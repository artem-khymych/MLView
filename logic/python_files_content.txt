файл experiment_manager.py
import inspect
import copy
import os
import pickle
from typing import Optional, Tuple

from PyQt5.QtCore import pyqtSignal, QObject, Qt
from PyQt5.QtWidgets import QMessageBox, QFileDialog, QPushButton, QLabel, QMainWindow, QWidget, QVBoxLayout, QComboBox
from sklearn.base import ClassifierMixin, RegressorMixin, is_regressor, is_classifier

from .evaluation.task_register import ModelTaskRegistry, NNModelType, TaskType
from .experiment.autoencoder_experiment import AutoencoderExperiment
from .experiment.experiment import Experiment
from .experiment.generic_nn_experiment import GenericNeuralNetworkExperiment
from .modules import models_manager, task_names
from ..ui.experiment_settings_dialog.experiment_comparison_dialog import ExperimentComparisonDialog
from ..ui.node import Node
from tensorflow.keras import layers, Model


class ExperimentManager(QObject):
    _instance = None  # Змінна класу для зберігання екземпляра
    get_all_task_experiments = pyqtSignal(object)
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        super().__init__()
        self.experiments = {}
        self.current_node = None
        self.current_model = None
        self.current_params = {}
        self.current_task = None

    def get_node(self, node: Node):
        self.current_node = node
        self._check_experiment_data()

    def update_name(self, id, name):
        self.experiments[id].name = name

    def get_ml_model(self, task, model, params):
        self.current_model = model
        self.current_params = params
        self.current_task = task
        self._check_experiment_data()

    def _check_experiment_data(self):
        if self.current_node is not None and self.current_model is not None and self.current_params is not None and self.current_task is not None:
            self.create_new_experiment()

    def create_nn_experiment(self, task, model_file_path, weights_file_path, load_type):
        self.current_params = {}
        model_params = {
            'optimizer': 'adam',  # Оптимізатор
            'learning_rate': 0.001,
            'loss': 'sparse_categorical_crossentropy',  # Функція втрат
            'metrics': ['accuracy'],  # Метрики
            'initial_epoch': 0,  # Початковий етап
            'trainable': True,  # Чи тренувати модель
            'activation': 'relu',  # Функція активації
        }

        # Параметри методу fit():
        fit_params = {
            'batch_size': 32,  # Розмір пакету
            'epochs': 10,  # Кількість етапів
            'callbacks': [],  # Колбеки для додаткової функціональності
            'validation_split': 0.2,  # Частина для валідації
            'shuffle': True,  # Перемішувати дані
            'initial_epoch': 0,  # Початковий етап
            'class_weight': {},  # Вага класів
        }

        self.current_params["model_params"] = model_params
        self.current_params["fit_params"] = fit_params
        model = ModelTaskRegistry().get_models_for_task(task_type=TaskType(task))

        if model == NNModelType.GENERIC:
            experiment = GenericNeuralNetworkExperiment(self.current_node.id, task, Model(), self.current_params,
                                                        load_type=load_type, weights_file=weights_file_path,
                                                        model_file=model_file_path)

        elif model == NNModelType.AUTOENCODER:
            task_spec_params = {
                "bottleneck_layer_index":1
            }
            if task == TaskType.ANOMALY_DETECTION.value:
                task_spec_params["threshold"] = 0.1

            self.current_params["task_spec_params"] = task_spec_params
            experiment = AutoencoderExperiment(self.current_node.id, task, Model(), self.current_params,
                                               load_type=load_type, weights_file=weights_file_path,
                                               model_file=model_file_path)

        self.experiments[self.current_node.id] = experiment
        print(f"Created new experiment with ID: {self.current_node.id}")

        self.current_node = None
        self.current_model = None
        self.current_params = None
        self.current_task = None

        return experiment

    def create_new_experiment(self):
        experiment = Experiment(self.current_node.id, self.current_task, self.current_model, self.current_params)
        self.experiments[self.current_node.id] = experiment
        print(f"Created new experiment with ID: {self.current_node.id}")

        self.current_node = None
        self.current_model = None
        self.current_params = None
        self.current_task = None

        return experiment

    def inherit_experiment_from(self, parent_id, child_id):
        if parent_id not in self.experiments:
            print(f"Error: Parent experiment with ID {parent_id} not found")
            return None

        parent_experiment = self.experiments[parent_id]

        child_experiment = type(parent_experiment)(
            id=child_id,
            task=parent_experiment.task,
            model=parent_experiment.model,
            params=copy.deepcopy(parent_experiment._params),
            parent=parent_experiment)
        child_experiment.__dict__.update(parent_experiment.__dict__)
        child_experiment.id = child_id
        child_experiment.parent = parent_experiment
        child_experiment.input_data_params = copy.deepcopy(parent_experiment.input_data_params)
        child_experiment.description = f"Успадковано від '{parent_experiment.name}'"
        child_experiment._name = f"Успадкований {parent_experiment.name}"
        child_experiment.is_finished = False
        if hasattr(child_experiment, "history"):
            child_experiment.history = None
        self.experiments[child_id] = child_experiment

        print(f"Inherited experiment created with ID: {child_id} from parent ID: {parent_id}")
        return child_experiment

    def get_experiment(self, experiment_id):
        if experiment_id in self.experiments:
            return self.experiments[experiment_id]
        return None

    def get_related_experiments(self, experiment_id):

        if experiment_id not in self.experiments:
            print(f"Experiment with ID {experiment_id} not found.")
            return []

        related_experiments = []
        main_experiment = self.experiments[experiment_id]

        if main_experiment.is_finished:
            related_experiments.append(main_experiment)

        parent = main_experiment.parent
        while parent is not None:
            if parent.is_finished:
                related_experiments.append(parent)
            parent = parent.parent

        def add_children(experiment):
            for child in experiment.children:
                if child.is_finished:
                    related_experiments.append(child)
                add_children(child)

        add_children(main_experiment)

        if main_experiment.parent:
            for sibling in main_experiment.parent.children:
                if sibling.id != experiment_id and sibling.is_finished:
                    related_experiments.append(sibling)
                    add_children(sibling)

        return related_experiments

    def show_comparison_dialog(self, experiment_id):

        related_experiments = self.get_related_experiments(experiment_id)

        if not related_experiments:
            print(f"No completed experiments related to ID {experiment_id} found.")
            return

        dialog = ExperimentComparisonDialog(related_experiments)
        dialog.exec_()

    def _save_ml_model(self, model, parent_widget):

        file_path, _ = QFileDialog.getSaveFileName(
            parent_widget,
            "Зберегти модель",
            "",
            "Pickle Files (*.pkl);;All Files (*)"
        )

        if file_path:
            try:
                if not file_path.endswith('.pkl'):
                    file_path += '.pkl'

                # Зберігаємо модель
                with open(file_path, 'wb') as f:
                    pickle.dump(model, f)

                QMessageBox.information(
                    parent_widget,
                    "Успіх",
                    f"Модель успішно збережено у файл:\n{file_path}"
                )

                return file_path
            except Exception as e:
                QMessageBox.critical(
                    parent_widget,
                    "Помилка",
                    f"Не вдалося зберегти модель. Помилка:\n{str(e)}"
                )
                return None
        else:
            return None

    def _save_nn_model(self, experiment, parent_widget):

        def save_keras_h5(model, parent_widget, experiment) -> Optional[Tuple[str, str]]:
            default_path = ""
            if experiment.model_file_path and experiment.model_file_path.endswith('.h5'):
                default_path = experiment.model_file_path

            file_path, _ = QFileDialog.getSaveFileName(
                parent_widget,
                "Зберегти модель Keras",
                default_path,
                "Keras Model (*.h5)"
            )

            if not file_path:
                return None

            if not file_path.endswith('.h5'):
                file_path += '.h5'

            model.save(file_path)
            return file_path, ""

        def save_tf_savedmodel(model, parent_widget, experiment) -> Optional[Tuple[str, str]]:
            # Визначаємо запропоновану директорію для збереження
            default_dir = ""
            if experiment.model_file_path:
                default_dir = os.path.dirname(experiment.model_file_path)

            dir_path = QFileDialog.getExistingDirectory(
                parent_widget,
                "Виберіть директорію для збереження моделі TensorFlow",
                default_dir
            )

            if not dir_path:
                return None

            model.save(dir_path, save_format='tf')
            return dir_path, ""

        def save_json_weights(model, parent_widget, experiment) -> Optional[Tuple[str, str]]:
            default_name = ""
            if experiment.model_file_path:
                default_dir = os.path.dirname(experiment.model_file_path)
                default_name = os.path.splitext(os.path.basename(experiment.model_file_path))[0]
            else:
                default_dir = ""

            json_path, _ = QFileDialog.getSaveFileName(
                parent_widget,
                "Зберегти структуру моделі (JSON)",
                os.path.join(default_dir, default_name) if default_name else "",
                "JSON Files (*.json)"
            )

            if not json_path:
                return None

            if not json_path.endswith('.json'):
                json_path += '.json'

            base_path = os.path.splitext(json_path)[0]
            weights_path = base_path + ".weights.h5"

            json_config = model.to_json()
            with open(json_path, 'w') as json_file:
                json_file.write(json_config)

            model.save_weights(weights_path)

            return json_path, weights_path

        def perform_save(save_format):
            try:
                model = experiment.model
                result = None

                if save_format == "Keras (.h5)":
                    result = save_keras_h5(model, parent_widget, experiment)
                elif save_format == "TensorFlow SavedModel":
                    result = save_tf_savedmodel(model, parent_widget, experiment)
                elif save_format == "JSON + Weights":
                    result = save_json_weights(model, parent_widget, experiment)

                if result:
                    model_path, weights_path = result
                    experiment.model_file_path = model_path
                    if weights_path:
                        experiment.weights_file_path = weights_path
                    experiment.load_type = save_format

                    QMessageBox.information(
                        parent_widget,
                        "Успіх",
                        f"Модель успішно збережено у форматі {save_format}"
                    )

                if save_window:
                    save_window.close()

                return result
            except Exception as e:
                QMessageBox.critical(
                    parent_widget,
                    "Помилка",
                    f"Не вдалося зберегти модель. Помилка:\n{str(e)}"
                )
                return None

        save_window = None
        if parent_widget is None:
            save_window = QMainWindow()
            save_window.setWindowTitle("Збереження моделі нейронної мережі")
            save_window.resize(500, 300)

            central_widget = QWidget()
            save_window.setCentralWidget(central_widget)
            layout = QVBoxLayout(central_widget)

            info_label = QLabel("Виберіть формат і місце збереження моделі нейронної мережі")
            info_label.setAlignment(Qt.AlignCenter)
            layout.addWidget(info_label)

            format_label = QLabel("Формат збереження:")
            layout.addWidget(format_label)

            format_combo = QComboBox()
            format_combo.addItems(["Keras (.h5)", "TensorFlow SavedModel", "JSON + Weights"])
            if experiment.load_type and experiment.load_type in ["Keras (.h5)", "TensorFlow SavedModel",
                                                                 "JSON + Weights"]:
                format_combo.setCurrentText(experiment.load_type)
            layout.addWidget(format_combo)

            save_button = QPushButton("Зберегти модель")
            layout.addWidget(save_button)

            # Показуємо вікно
            save_window.show()
            parent_widget = save_window

            def on_save_button_clicked():
                save_format = format_combo.currentText()
                perform_save(save_format)

            save_button.clicked.connect(on_save_button_clicked)

        else:
            formats = ["Keras (.h5)", "TensorFlow SavedModel", "JSON + Weights"]
            selected_format = experiment.load_type if experiment.load_type in formats else formats[0]

            perform_save(selected_format)

    def save_model(self, experiment, parent_widget=None):
        if isinstance(experiment, GenericNeuralNetworkExperiment):
            self._save_nn_model(experiment, parent_widget)
        else:
            self._save_ml_model(experiment.model, parent_widget)

    def get_experiments_by_task(self, task_type):

        matching_experiments = []

        if hasattr(task_type, 'value'):
            task_value = task_type.value
        else:
            task_value = task_type

        for experiment_id, experiment in self.experiments.items():
            experiment_task = experiment.task

            if (hasattr(experiment_task, 'value') and experiment_task.value == task_value) or \
                    (experiment_task == task_value) or \
                    (str(experiment_task) == str(task_value)):
                matching_experiments.append(experiment)

        self.get_all_task_experiments.emit(matching_experiments)
        return matching_experiments

import numpy as np
import tensorflow as tf
from typing import Dict, Any
from project.logic.experiment.nn_experiment import NeuralNetworkExperiment
from project.logic.evaluation.task_register import TaskType, NNMetricFactory, NNModelType


class AutoencoderExperiment(NeuralNetworkExperiment):
    """
    Autoencoder experiment class with support for dimensionality reduction and anomaly detection.
    """

    def __init__(self, id: int, task, model: Any, params: Dict[str, Any], parent=None, load_type="", model_file="",
                 weights_file=""):
        super().__init__(id, task, model, params, parent, load_type=load_type, model_file=model_file,
                         weights_file=weights_file)

        self.task_spec_params = params.get('task_spec_params', {})
        self.bottleneck_layer_index = self.task_spec_params.get('bottleneck_layer_index', None)
        self.anomaly_threshold = self.task_spec_params.get('anomaly_threshold', 0.1)

        # Encoder model for obtaining latent representations
        self.encoder_model = None

        # Saved latent representations
        self.latent_train = None
        self.latent_test = None

        # Reconstructed data
        self.reconstructed_train = None
        self.reconstructed_test = None

        # Anomaly scores
        self.anomaly_scores_train = None
        self.anomaly_scores_test = None
        self.metric_strategy = NNMetricFactory().create_metric(NNModelType.AUTOENCODER, TaskType(self.task))

    def _create_encoder_model(self) -> None:
        """
        Creates an encoder model from the full autoencoder model.
        Uses bottleneck_layer_index to determine the final encoder layer.
        """
        try:
            # If bottleneck layer index is not specified, try to determine it automatically
            if self.bottleneck_layer_index is None:
                # Find the smallest layer (with fewest neurons)
                min_neurons = float('inf')
                for i, layer in enumerate(self.model.layers):
                    if hasattr(layer, 'output_shape'):
                        output_shape = layer.output_shape
                        if isinstance(output_shape, tuple) and len(output_shape) >= 2:
                            neurons = np.prod(output_shape[1:])
                            if neurons < min_neurons:
                                min_neurons = neurons
                                self.bottleneck_layer_index = i

                if self.bottleneck_layer_index is None:
                    # If not found, use the middle layer
                    self.bottleneck_layer_index = len(self.model.layers) // 2

                print(f"Automatically determined bottleneck layer: {self.bottleneck_layer_index}")

            # Create encoder model that outputs from the bottleneck layer
            bottleneck_layer = self.model.layers[self.bottleneck_layer_index]
            self.encoder_model = tf.keras.Model(
                inputs=self.model.input,
                outputs=bottleneck_layer.output
            )

            print(f"Created encoder from layer {self.bottleneck_layer_index}")
        except Exception as e:
            print(f"Error creating encoder model: {str(e)}")
            self.encoder_model = None

    def run(self) -> None:
        """
        Run the autoencoder experiment.
        """
        import time

        self._load_data()
        self._validate_data()

        try:
            self.X_train = self._convert_to_tensorflow_compatible(self.X_train)
            self.X_test = self._convert_to_tensorflow_compatible(self.X_test)
        except Exception as e:
            raise ValueError(f"Error converting data to TensorFlow format: {str(e)}")

        # Start time measurement
        start_time = time.time()

        self.load_model_from_file()
        model = self.model
        # Model compilation
        if isinstance(model, tf.keras.Model) or isinstance(model, tf.keras.Sequential):
            self._compile_model(model)

        # Model training
        if self.X_train is not None:
            try:
                # For autoencoder, target data = input data
                self.history = self._train_autoencoder(model)
            except Exception as e:
                raise ValueError(f"Error training autoencoder: {str(e)}")

        # Process results
        if self.task == TaskType.DIMENSIONALITY_REDUCTION:
            self._process_dimensionality_reduction()
        elif self.task == TaskType.ANOMALY_DETECTION:
            self._process_anomaly_detection()

        # Save trained model
        self.trained_model = model
        self.train_time = time.time() - start_time
        self.is_finished = True
        self.experiment_finished.emit(self.train_time)

    def _train_autoencoder(self, model) -> tf.keras.callbacks.History:
        """
        Train the autoencoder.
        """
        # Default training parameters
        default_fit_params = {
            'x': self.X_train,
            'y': self.X_train,  # For autoencoder, target data = input data
            'batch_size': 32,
            'epochs': 50,
            'verbose': 1,
            'validation_split': 0.2,
            'shuffle': True
        }

        # Update training parameters
        fit_params = {**default_fit_params}

        # Update with training parameters from self._params
        if 'fit_params' in self._params:
            for key, value in self._params.get('fit_params', {}).items():
                if value is not None:
                    fit_params[key] = value

        # Remove None parameters
        for key in list(fit_params.keys()):
            if fit_params[key] is None:
                del fit_params[key]

        # Train model
        history = model.fit(**fit_params)
        return history

    def _process_dimensionality_reduction(self) -> None:
        """
        Process results for dimensionality reduction task.
        """
        try:
            # Verify encoder was created
            if self.encoder_model is None:
                self._create_encoder_model()

            if self.encoder_model is None:
                raise ValueError("Failed to create encoder model")

            # Get latent representations
            self.latent_train = self.encoder_model.predict(self.X_train)
            self.latent_test = self.encoder_model.predict(self.X_test)

            # Save reconstructed data for quality assessment
            self.reconstructed_train = self.model.predict(self.X_train)
            self.reconstructed_test = self.model.predict(self.X_test)

            # Add dimensionality information
            print(f"Data successfully processed for dimensionality reduction task.")
            print(
                f"Compression: {self.data_info['original_dim']} -> {self.data_info['latent_dim']} (ratio: {self.data_info['compression_ratio']:.2f}x)")

        except Exception as e:
            print(f"Error processing data for dimensionality reduction: {str(e)}")

    def _process_anomaly_detection(self) -> None:
        """
        Process results for anomaly detection task.
        """
        try:
            # Get reconstructed data
            self.reconstructed_train = self.model.predict(self.X_train)
            self.reconstructed_test = self.model.predict(self.X_test)

            # Calculate reconstruction errors (per sample)
            reconstruction_errors_train = self._calculate_reconstruction_errors(self.X_train, self.reconstructed_train)
            reconstruction_errors_test = self._calculate_reconstruction_errors(self.X_test, self.reconstructed_test)

            # Save anomaly scores
            self.anomaly_scores_train = reconstruction_errors_train
            self.anomaly_scores_test = reconstruction_errors_test

            # Determine anomaly threshold if not set
            if self.anomaly_threshold is None:
                # Use 95th percentile of training errors
                self.anomaly_threshold = np.percentile(reconstruction_errors_train, 95)
                print(f"Automatically determined anomaly threshold: {self.anomaly_threshold:.4f}")

            # Detect anomalies
            self.train_predictions = (reconstruction_errors_train > self.anomaly_threshold).astype(int)
            self.test_predictions = (reconstruction_errors_test > self.anomaly_threshold).astype(int)

            # If labels (ground truth) exist, use them for evaluation
            if hasattr(self, 'y_train') and self.y_train is not None:
                self.train_actual = self.y_train
                self.test_actual = self.y_test
            else:
                # If no labels, assume all training data is normal
                self.train_actual = np.zeros(len(self.X_train))
                self.test_actual = np.zeros(len(self.X_test))
                print("Warning: Anomaly labels not provided. Assuming all training data is normal.")

            print(f"Data successfully processed for anomaly detection task.")

            # Add information about detected anomalies
            anomalies_train = np.sum(self.train_predictions)
            anomalies_test = np.sum(self.test_predictions)

        except Exception as e:
            print(f"Error processing data for anomaly detection: {str(e)}")

    def _calculate_reconstruction_errors(self, original: np.ndarray, reconstructed: np.ndarray) -> np.ndarray:
        """
        Calculate reconstruction errors per sample.

        Args:
            original: Original data
            reconstructed: Reconstructed data

        Returns:
            Array of reconstruction errors per sample
        """
        tf.experimental.numpy.experimental_enable_numpy_behavior()
        # Flatten data for simpler calculations
        original_flat = original.reshape(len(original), -1)
        reconstructed_flat = reconstructed.reshape(len(reconstructed), -1)

        # Calculate MSE per sample
        mse = np.mean(np.square(original_flat - reconstructed_flat), axis=1)

        return mse

    def _compile_model(self, model) -> None:
        """
        Compile the autoencoder model.
        """
        # Default compilation parameters
        default_compile_params = {
            'optimizer': 'adam',
            'loss': 'mse',
            'metrics': ['mae']
        }

        # Update default parameters with provided ones
        compile_params = {**default_compile_params}

        # Update with model parameters from self._params
        if 'model_params' in self._params:
            model_params = self._params.get('model_params', {})
            for key in ['loss', 'metrics']:
                if key in model_params and model_params[key] is not None:
                    compile_params[key] = model_params[key]

            # Handle optimizer with learning_rate
            if 'optimizer' in model_params and model_params['optimizer'] is not None:
                optimizer_name = model_params['optimizer']
                learning_rate = model_params.get('learning_rate')

                # Create optimizer object with specified learning_rate
                if learning_rate is not None:
                    if optimizer_name == 'adam':
                        compile_params['optimizer'] = tf.keras.optimizers.Adam(learning_rate=learning_rate)
                    elif optimizer_name == 'sgd':
                        compile_params['optimizer'] = tf.keras.optimizers.SGD(learning_rate=learning_rate)
                    elif optimizer_name == 'rmsprop':
                        compile_params['optimizer'] = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)
                    else:
                        compile_params['optimizer'] = optimizer_name
                else:
                    compile_params['optimizer'] = optimizer_name

        # Compile model
        try:
            model.compile(**compile_params)
            print(f"Autoencoder model compiled with parameters: {compile_params}")
        except Exception as e:
            raise ValueError(f"Error compiling autoencoder model: {str(e)}")

    def _validate_data(self) -> None:
        """
        Validate data compatibility with task type and model.
        """
        if self.X_train is None or self.X_test is None:
            raise ValueError("Missing input data for autoencoder")

        # Convert to numpy arrays
        if not isinstance(self.X_train, np.ndarray):
            try:
                self.X_train = np.array(self.X_train)
            except Exception as e:
                raise ValueError(f"Failed to convert X_train to numpy array: {str(e)}")

        if not isinstance(self.X_test, np.ndarray):
            try:
                self.X_test = np.array(self.X_test)
            except Exception as e:
                raise ValueError(f"Failed to convert X_test to numpy array: {str(e)}")

        # Check for NaN values
        if np.isnan(self.X_train).any():
            raise ValueError("NaN values detected in training data")
        if np.isnan(self.X_test).any():
            raise ValueError("NaN values detected in test data")

        # Check dimension compatibility
        if self.X_train.ndim != self.X_test.ndim:
            raise ValueError(f"Dimension mismatch between X_train ({self.X_train.ndim}) and X_test ({self.X_test.ndim})")

        # For anomaly detection task, check labels (if available)
        if self.task == TaskType.ANOMALY_DETECTION and hasattr(self, 'y_train') and self.y_train is not None:
            try:
                if not isinstance(self.y_train, np.ndarray):
                    self.y_train = np.array(self.y_train)
                if not isinstance(self.y_test, np.ndarray):
                    self.y_test = np.array(self.y_test)

                # Verify labels are binary (0 - normal, 1 - anomaly)
                unique_labels = np.unique(np.concatenate([self.y_train, self.y_test]))
                if not np.array_equal(unique_labels, np.array([0, 1])) and not np.array_equal(unique_labels, np.array(
                        [0])) and not np.array_equal(unique_labels, np.array([1])):
                    print(
                        f"Warning: Anomaly labels should be 0 (normal) or 1 (anomaly). Detected labels: {unique_labels}")

                    # Convert labels to binary
                    if len(unique_labels) == 2:
                        label_map = {unique_labels[0]: 0, unique_labels[1]: 1}
                        self.y_train = np.array([label_map[label] for label in self.y_train])
                        self.y_test = np.array([label_map[label] for label in self.y_test])
                        print(f"Labels converted: {unique_labels} -> [0, 1]")

                # Save anomaly distribution information
                anomalies_train = np.sum(self.y_train)
                anomalies_test = np.sum(self.y_test)
                self.data_info['true_anomalies_train'] = anomalies_train
                self.data_info['true_anomalies_test'] = anomalies_test
                self.data_info['true_anomalies_train_percent'] = (anomalies_train / len(self.y_train)) * 100
                self.data_info['true_anomalies_test_percent'] = (anomalies_test / len(self.y_test)) * 100

                print(f"Actual anomalies in data:")
                print(
                    f"  Training data: {anomalies_train} out of {len(self.y_train)} ({self.data_info['true_anomalies_train_percent']:.2f}%)")
                print(
                    f"  Test data: {anomalies_test} out of {len(self.y_test)} ({self.data_info['true_anomalies_test_percent']:.2f}%)")

            except Exception as e:
                print(f"Error processing anomaly labels: {str(e)}")

    def evaluate(self) -> None:
        """
        Evaluate autoencoder experiment results.
        """
        if not self.is_finished:
            raise RuntimeError("Experiment must be completed before evaluation")

        # Initialize metric dictionaries
        self.train_metrics = {}
        self.test_metrics = {}

        try:
            if TaskType(self.task) == TaskType.DIMENSIONALITY_REDUCTION:
                self.train_metrics = self.metric_strategy.evaluate(self.X_train, self.reconstructed_train)
                self.test_metrics = self.metric_strategy.evaluate(self.X_test, self.reconstructed_test)
            elif TaskType(self.task) == TaskType.ANOMALY_DETECTION:
                self.train_metrics = self.metric_strategy.evaluate(self.X_train, self.train_predictions)
                self.test_metrics = self.metric_strategy.evaluate(self.X_test, self.test_predictions)
            else:
                raise ValueError(f"Unsupported task type for autoencoder: {self.task}")

        except Exception as e:
            print(f"Error evaluating results: {str(e)}")
            self.train_metrics = {"error": f"Evaluation error: {str(e)}"}
            self.test_metrics = {"error": f"Evaluation error: {str(e)}"}

        # Signal evaluation completion
        self.experiment_evaluated.emit(self.train_metrics, self.test_metrics)

файл experiment.py
from typing import Dict
from collections.abc import Callable

from PyQt5.QtCore import pyqtSignal, QObject
from PyQt5.QtWidgets import QMessageBox
from pandas import DataFrame
from sklearn.neural_network import MLPClassifier, MLPRegressor

from project.logic.evaluation.metric_strategies.anomaly_detection_metric import AnomalyDetectionMetric
from project.logic.evaluation.metric_strategies.classification_metric import ClassificationMetric
from project.logic.evaluation.metric_strategies.clustering_metric import ClusteringMetric
from project.logic.evaluation.metric_strategies.density_estimation_metric import DensityEstimationMetric
from project.logic.evaluation.metric_strategies.dim_reduction_metric import DimReduction
from project.logic.evaluation.metric_strategies.metric_strategy import MetricStrategy, TimeSeriesMetric
from project.logic.evaluation.metric_strategies.regression_metric import RegressionMetric
from project.logic.experiment.input_data_params import InputDataParams
from project.logic.modules import task_names
import time
import pandas as pd
from sklearn.model_selection import train_test_split


class Experiment(QObject):
    experiment_finished = pyqtSignal(float)
    experiment_evaluated = pyqtSignal(object, object)

    def __init__(self, id, task, model, params, parent=None):
        super().__init__()
        self.id: int = id
        self.task: str = task
        self._name = f"Експеримент {id}"  # "Experiment {id}" -> "Експеримент {id}"
        self.description = ""

        self.model: Callable[[Dict]] = model
        self.trained_model = None
        self._params: Dict[str:any] = params
        self.input_data_params = InputDataParams()

        # Storing data in classic format
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None

        self.train_time: float = 0
        self.is_finished: bool = False
        self.metric_strategy: MetricStrategy

        # For storing results
        self.train_predictions = None
        self.test_predictions = None
        self.train_actual = None
        self.test_actual = None

        # For storing transformation results
        self.transformed_train = None
        self.transformed_test = None

        self.train_metrics = None
        self.test_metrics = None

        self.parent = parent
        self.children = []

    @property
    def name(self) -> str:
        return self._name

    @name.setter
    def name(self, new_name):
        if not isinstance(new_name, str):
            raise ValueError("Назва має бути рядком")  # "Name must be a string" -> "Назва має бути рядком"
        self._name = new_name

    @property
    def params(self):
        return self._params

    @params.setter
    def params(self, new_params):
        self._params = new_params

    def _choose_metric_strategy(self) -> MetricStrategy:
        # Get metric strategy for standard ML tasks
        if self.task == task_names.CLASSIFICATION:
            self.metric_strategy = ClassificationMetric()
        elif self.task == task_names.REGRESSION:
            self.metric_strategy = RegressionMetric()
        elif self.task == task_names.CLUSTERING:
            self.metric_strategy = ClusteringMetric()
        elif self.task == task_names.ANOMALY_DETECTION:
            self.metric_strategy = AnomalyDetectionMetric()
        elif self.task == task_names.DENSITY_ESTIMATION:
            self.metric_strategy = DensityEstimationMetric()
        elif self.task == task_names.DIMENSIONALITY_REDUCTION:
            self.metric_strategy = DimReduction()
        elif self.task == task_names.TIME_SERIES:
            self.metric_strategy = TimeSeriesMetric()

        # Get metric strategy if we have an sklearn neural network task
        if self.task == task_names.MLP:
            self.metric_strategy = self.__get_metric_strategy_for_mlp()

    def __get_metric_strategy_for_mlp(self) -> MetricStrategy:
        if isinstance(self.model, MLPClassifier):
            self.task = task_names.CLASSIFICATION
            return ClassificationMetric()
        elif self.model == MLPRegressor():
            self.task = task_names.REGRESSION
            return RegressionMetric()
        else:
            raise TypeError(f"Unrecognized model: {self.model}")

    def run(self):
        """
        Runs a machine learning experiment with the given input parameters.
        Processes data, trains the model, and evaluates results on training and test sets.
        """
        try:
            # Loading and preparing data
            self._load_data()

            # Choosing the appropriate metric for model evaluation
            self._choose_metric_strategy()

            # Training the model and measuring time
            start_time = time.time()

            # Creating a model instance with the given parameters
            model_instance = type(self.model)(**self._params)

            # Different training logic depending on the task type
            if self.task in [task_names.CLASSIFICATION, task_names.REGRESSION, task_names.MLP]:
                # Training the model
                model_instance.fit(self.X_train, self.y_train)

                # Saving results (for both training and test sets)
                self.train_predictions = model_instance.predict(self.X_train)
                self.test_predictions = model_instance.predict(self.X_test)
                self.train_actual = self.y_train
                self.test_actual = self.y_test

            elif self.task in [task_names.CLUSTERING, task_names.ANOMALY_DETECTION, task_names.DENSITY_ESTIMATION]:
                # For unsupervised tasks, target variable is not needed
                model_instance.fit(self.X_train)

                # Getting results for both sets
                if hasattr(model_instance, 'predict'):
                    self.train_predictions = model_instance.predict(self.X_train)
                    self.test_predictions = model_instance.predict(self.X_test)
                elif hasattr(model_instance, 'fit_predict') and (
                        self.task == task_names.CLUSTERING or self.task == task_names.ANOMALY_DETECTION):
                    # For some clustering algorithms
                    # For training set, use results from fit
                    self.train_predictions = model_instance.labels_ if hasattr(model_instance,
                                                                               'labels_') else model_instance.fit_predict(
                        self.X_train)
                    # For test set
                    self.test_predictions = model_instance.fit_predict(self.X_test)

            elif self.task == task_names.DIMENSIONALITY_REDUCTION:
                # Training dimensionality reduction model

                # Transforming data for both sets
                self.transformed_train = model_instance.fit_transform(self.X_train, y=None)
                self.transformed_test = model_instance.fit_transform(self.X_test, y=None)

            elif self.task == task_names.TIME_SERIES:
                # Specific processing for time series
                # Logic depends on specific implementation and model
                pass
            elif self.task == task_names.OWN_NN:
                pass

            # Saving training time
            self.train_time = time.time() - start_time

            # Saving the trained model
            self.trained_model = model_instance

            # Marking the experiment as finished
            self.is_finished = True
            self.experiment_finished.emit(self.train_time)
        except Exception as e:
            QMessageBox.warning(None, "Помилка параметрів", f"Виникла помилка у налаштованих параметрах:\n {e}")  # "Invalid parameters" -> "Помилка параметрів", "An error occurred in the configured parameters" -> "Виникла помилка у налаштованих параметрах"
        return

    def evaluate(self):
        self.train_metrics, self.test_metrics = self._calculate_metrics()
        print("Train metrics:", self.train_metrics)
        print("Test metrics:", self.test_metrics)
        self.experiment_evaluated.emit(self.train_metrics, self.test_metrics)

    def _load_data(self):
        """
        Loads data based on input data parameters.
        Processes categorical variables and splits data into X_train, X_test, y_train, y_test.
        """
        params = self.input_data_params

        if params.mode == 'single_file':
            # Loading from a single file
            data = self._load_file(params.single_file_path)

            # Data processing before splitting
            if not params.is_target_not_required():
                # For supervised learning
                X = data.drop(params.target_variable, axis=1)
                y = data[params.target_variable]

                # Splitting into training and test sets
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y,
                    test_size=params.test_percent / 100,
                    random_state=params.seed,
                    stratify=y if self.task == task_names.CLASSIFICATION else None
                )

                # Processing categorical variables for X_train and X_test
                X_combined = pd.concat([X_train, X_test])
                X_combined_encoded = self._encode_categorical_variables(X_combined)

                # Splitting back after processing
                self.X_train = X_combined_encoded.iloc[:len(X_train)]
                self.X_test = X_combined_encoded.iloc[len(X_train):]

                # Processing y if it's a categorical variable and the task is classification
                if self.task == task_names.CLASSIFICATION and y.dtype == 'object':
                    y_combined = pd.concat([y_train, y_test])
                    y_combined_encoded = self._encode_categorical_variables(pd.DataFrame(y_combined))

                    self.y_train = y_combined_encoded.iloc[:len(y_train)].values.ravel()
                    self.y_test = y_combined_encoded.iloc[len(y_train):].values.ravel()
                else:
                    self.y_train = y_train
                    self.y_test = y_test

            else:
                # For unsupervised learning
                # Splitting into training and test sets
                X_train, X_test = train_test_split(
                    data,
                    test_size=params.test_percent / 100,
                    random_state=params.seed
                )

                # Processing categorical variables for X_train and X_test
                X_combined = pd.concat([X_train, X_test])
                X_combined_encoded = self._encode_categorical_variables(X_combined)

                # Splitting back after processing
                self.X_train = X_combined_encoded.iloc[:len(X_train)]
                self.X_test = X_combined_encoded.iloc[len(X_train):]

                # For unsupervised learning, y is not needed
                self.y_train = None
                self.y_test = None
        else:
            # Loading from separate files
            X_train = self._load_file(params.x_train_file_path)
            X_test = self._load_file(params.x_test_file_path)

            # Processing categorical variables (joint processing for encoding consistency)
            X_combined = pd.concat([X_train, X_test])
            X_combined_encoded = self._encode_categorical_variables(X_combined)

            # Splitting back after processing
            self.X_train = X_combined_encoded.iloc[:len(X_train)]
            self.X_test = X_combined_encoded.iloc[len(X_train):]

            if not params.is_target_not_required():
                # For supervised learning
                y_train = self._load_file(params.y_train_file_path)
                y_test = self._load_file(params.y_test_file_path)

                # Checking if it's a DataFrame and processing accordingly
                if isinstance(y_train, pd.DataFrame):
                    if len(y_train.columns) == 1:
                        y_train = y_train.iloc[:, 0]
                    else:
                        # If multiple columns, process the first one
                        y_train = y_train.iloc[:, 0]

                if isinstance(y_test, pd.DataFrame):
                    if len(y_test.columns) == 1:
                        y_test = y_test.iloc[:, 0]
                    else:
                        # If multiple columns, process the first one
                        y_test = y_test.iloc[:, 0]

                # Processing categorical variables for y if necessary
                if self.task == task_names.CLASSIFICATION and pd.api.types.is_object_dtype(y_train):
                    y_combined = pd.concat([y_train, y_test])
                    y_combined = pd.DataFrame(y_combined, columns=['target'])
                    y_combined_encoded = self._encode_categorical_variables(y_combined)

                    self.y_train = y_combined_encoded.iloc[:len(y_train)].values.ravel()
                    self.y_test = y_combined_encoded.iloc[len(y_train):].values.ravel()
                else:
                    self.y_train = y_train
                    self.y_test = y_test
            else:
                # For unsupervised learning
                self.y_train = None
                self.y_test = None

    def _load_file(self, file_path):
        """
        Loads data from files of different formats.
        Supported formats: CSV, Excel, JSON, Parquet.

        Args:
            file_path (str): Path to the data file

        Returns:
            DataFrame: Loaded data as a pandas DataFrame
        """
        file_extension = file_path.split('.')[-1].lower()

        if file_extension == 'csv':
            return pd.read_csv(
                file_path,
                encoding=self.input_data_params.file_encoding,
                sep=self.input_data_params.file_separator
            )
        elif file_extension in ['xlsx', 'xls']:
            return pd.read_excel(file_path)
        elif file_extension == 'json':
            return pd.read_json(file_path)
        elif file_extension == 'parquet':
            return pd.read_parquet(file_path)
        else:
            raise ValueError(f"Непідтримуваний формат файлу: {file_extension}")  # "Unsupported file format" -> "Непідтримуваний формат файлу"

    def _encode_categorical_variables(self, data):
        """
        Processes categorical variables in a DataFrame according to the selected encoding method.

        Args:
            data (DataFrame): Data to process

        Returns:
            DataFrame: Data with processed categorical variables
        """
        categorical_columns = data.select_dtypes(include=['object', 'category']).columns

        if len(categorical_columns) == 0:
            return data

        result_data = data.copy()

        # Selecting the categorical variable encoding method
        encoding_method = self.input_data_params.categorical_encoding

        if encoding_method == 'one-hot':
            # One-hot encoding (all categories as separate columns)
            for column in categorical_columns:
                # Creating one-hot encoding
                one_hot = pd.get_dummies(
                    result_data[column],
                    prefix=column,
                    drop_first=False
                )

                # Removing the original column and adding encoded columns
                result_data = pd.concat([result_data.drop(column, axis=1), one_hot], axis=1)

        elif encoding_method == 'to_categorical':
            from sklearn.preprocessing import LabelEncoder

            for column in categorical_columns:
                # Creating an encoder and training it on the data
                label_encoder = LabelEncoder()
                result_data[column] = label_encoder.fit_transform(result_data[column])

        return result_data

    def _calculate_metrics(self):
        """
        Calculates model performance metrics for training and test sets.

        Returns:
            tuple: (train_metrics, test_metrics) - tuple with metric dictionaries for each set
        """
        if not self.is_finished:
            raise BlockingIOError("The experiment isn't finished")

        train_metrics = {}
        test_metrics = {}

        if self.task in [task_names.CLASSIFICATION, task_names.REGRESSION, task_names.MLP]:
            # Metrics for training set
            train_metrics = self.metric_strategy.evaluate(self.train_actual, self.train_predictions)
            # Metrics for test set
            test_metrics = self.metric_strategy.evaluate(self.test_actual, self.test_predictions)

        elif self.task == task_names.CLUSTERING:
            # For clustering, internal metrics can be used
            train_metrics = self.metric_strategy.evaluate(self.X_train, self.train_predictions)
            test_metrics = self.metric_strategy.evaluate(self.X_test, self.test_predictions)

        elif self.task == task_names.DIMENSIONALITY_REDUCTION:
            # For dimensionality reduction
            train_metrics = self.metric_strategy.evaluate(self.X_train, self.transformed_train)
            test_metrics = self.metric_strategy.evaluate(self.X_test, self.transformed_test)

        elif self.task == task_names.ANOMALY_DETECTION:
            # For anomaly detection
            train_metrics = self.metric_strategy.evaluate(self.X_train, self.train_predictions)
            test_metrics = self.metric_strategy.evaluate(self.X_test, self.test_predictions)

        elif self.task == task_names.DENSITY_ESTIMATION:
            # For density estimation
            train_metrics = self.metric_strategy.evaluate(self.X_train, self.trained_model)
            test_metrics = self.metric_strategy.evaluate(self.X_test, self.trained_model)

        elif self.task == task_names.TIME_SERIES:
            # For time series (if train/test split is possible)
            if self.train_actual is not None and self.train_predictions is not None:
                train_metrics = self.metric_strategy.evaluate(self.train_actual, self.train_predictions)
            if self.test_actual is not None and self.test_predictions is not None:
                test_metrics = self.metric_strategy.evaluate(self.test_actual, self.test_predictions)

        # If metrics are empty, add an error message
        if not train_metrics:
            train_metrics = {"error": "Неможливо обчислити метрики для тренувального набору"}  # "Cannot calculate metrics for training set" -> "Неможливо обчислити метрики для тренувального набору"
        if not test_metrics:
            test_metrics = {"error": "Неможливо обчислити метрики для тестового набору"}  # "Cannot calculate metrics for test set" -> "Неможливо обчислити метрики для тестового набору"

        return train_metrics, test_metrics

    def get_params_for_tune(self):
        if self.input_data_params.single_file_path or (self.input_data_params.x_train_file_path and self.input_data_params.y_train_file_path):
            try:
                self._load_data()
            except Exception as e:
                QMessageBox.warning(None, "Помилка параметрів", f"Виникла помилка у налаштованих параметрах:\n {e}")  # "Invalid parameters" -> "Помилка параметрів", "An error occurred in the configured parameters" -> "Виникла помилка у налаштованих параметрах"
                return None, None
            return self.X_train, self.y_train
        else:
            return None, None

файл generic_nn_experiment.py
import time
from typing import Dict, Any, Optional
import tensorflow as tf
from PyQt5.QtWidgets import QMessageBox
import numpy as np

from project.logic.experiment.experiment import Experiment
from project.logic.experiment.nn_experiment import NeuralNetworkExperiment
from project.logic.experiment.nn_input_data_params import NeuralNetInputDataParams
from project.logic.evaluation.task_register import TaskType, NNMetricFactory, get_nn_metric, \
    NNModelType


class GenericNeuralNetworkExperiment(NeuralNetworkExperiment):
    """
    Neural network experiment class with support for different architectures
    and saving specific hyperparameters for each task type.
    """

    def __init__(self, id: int, task, model: Any, params: Dict[str, Any], parent=None, load_type="", model_file="",
                 weights_file=""):
        super().__init__(id, task, model, params, parent, load_type=load_type, model_file=model_file,
                         weights_file=weights_file)

        # Training history
        self.history = None
        # Model file paths
        self.input_data_params: NeuralNetInputDataParams = NeuralNetInputDataParams()

        # Model task registry
        self.metric_factory = NNMetricFactory()

        # For storing data information
        self.data_info = {}

    def get_params_for_tune(self):
        self._load_data()

        self._validate_data()
        try:
            self.X_train = self._convert_to_tensorflow_compatible(self.X_train)
            self.X_test = self._convert_to_tensorflow_compatible(self.X_test)
            if self.y_train is not None:
                self.y_train = self._convert_to_tensorflow_compatible(self.y_train)
            if self.y_test is not None:
                self.y_test = self._convert_to_tensorflow_compatible(self.y_test)
        except Exception as e:
            raise ValueError(f"Помилка при перетворенні даних в формат TensorFlow: {str(e)}")

        return self.X_train, self.y_train

    def run(self) -> None:
        """
        Run neural network experiment.
        """
        try:
            self._load_data()

            self._validate_data()
            try:
                self.X_train = self._convert_to_tensorflow_compatible(self.X_train)
                self.X_test = self._convert_to_tensorflow_compatible(self.X_test)
                if self.y_train is not None:
                    self.y_train = self._convert_to_tensorflow_compatible(self.y_train)
                if self.y_test is not None:
                    self.y_test = self._convert_to_tensorflow_compatible(self.y_test)
            except Exception as e:
                raise ValueError(f"Помилка при перетворенні даних в формат TensorFlow: {str(e)}")

            # Start time measurement
            start_time = time.time()

            if self.model_file_path:
                # If model is loaded from file
                self.load_model_from_file()
            model = self.model

            # Compile model (if it's a Keras model)
            if isinstance(model, tf.keras.Model) or isinstance(model, tf.keras.Sequential):
                self._compile_model(model)

            # Train model (if needed)
            if self.X_train is not None:
                try:
                    self.history = self._train_model(model)
                except Exception as e:
                    error_msg = str(e)
                    if "SparseSoftmaxCrossEntropyWithLogits" in error_msg and "valid range" in error_msg:
                        raise ValueError(
                            f"Помилка при навчанні: значення міток виходять за межі допустимого діапазону.\n"
                            f"Перевірте формат міток та їх відповідність кількості класів у моделі.\n"
                            f"Деталі: {error_msg}"
                        )
                    else:
                        raise

            # Predictions
            if self.task == TaskType.REGRESSION:
                self.train_predictions = model.predict(self.X_train)
                self.test_predictions = model.predict(self.X_test)
                self.train_actual = self.y_train
                self.test_actual = self.y_test
            elif self.task == TaskType.CLASSIFICATION:
                train_probabilities = model.predict(self.X_train)
                test_probabilities = model.predict(self.X_test)

                self.train_predictions = self._convert_probabilities_to_classes(train_probabilities)
                self.test_predictions = self._convert_probabilities_to_classes(test_probabilities)

                self.train_actual = self.y_train
                self.test_actual = self.y_test
            elif self.task == TaskType.TIME_SERIES_FORECASTING:
                # Time series require special processing
                self._process_time_series_predictions(model)
            # Save trained model
            self.trained_model = model
            self.train_time = time.time() - start_time
            self.is_finished = True
            self.experiment_finished.emit(self.train_time)
        except Exception as e:
            QMessageBox.warning(None, "Хибні параметри", f"Винила помилка у налаштованих параметрах:\n {e}")
        return

    def _process_time_series_predictions(self, model):

        try:
            x_train = self._convert_to_tensorflow_compatible(self.X_train)
            x_test = self._convert_to_tensorflow_compatible(self.X_test)

            if isinstance(x_train, (np.ndarray, tf.Tensor)) and len(x_train.shape) == 3:
                self.train_predictions = model.predict(x_train)
                self.test_predictions = model.predict(x_test)
                self.train_actual = self.y_train
                self.test_actual = self.y_test
            else:
                raise NotImplementedError("Processing for this data type is not implemented")
        except Exception as e:
            print(f"Error processing time series: {str(e)}")

    def _validate_data(self):
        """
        Check data correspondence to task type and model.
        """
        if self.task == TaskType.CLASSIFICATION:
            # Validation for classification tasks
            self._validate_classification_data()
        elif self.task == TaskType.REGRESSION:
            # Validation for regression tasks
            self._validate_regression_data()
        elif self.task == TaskType.TIME_SERIES_FORECASTING:
            # Validation for time series forecasting tasks
            self._validate_time_series_data()

        # Store data information
        self._store_data_info()

    def _validate_classification_data(self):
        """
        Validate data for classification task.
        """
        if self.y_train is None or self.y_test is None:
            raise ValueError("Class labels are required for classification")

        # Check that labels are integers for sparse_categorical_crossentropy
        if not isinstance(self.y_train, np.ndarray):
            self.y_train = np.array(self.y_train)
        if not isinstance(self.y_test, np.ndarray):
            self.y_test = np.array(self.y_test)

        # Label validation
        unique_labels = np.unique(np.concatenate([self.y_train, self.y_test]))
        num_classes = len(unique_labels)

        if num_classes == 2:
            # Binary classification
            if np.min(unique_labels) < 0 or np.max(unique_labels) > 1:
                # Transform labels for binary classification (0 and 1)
                self._transform_binary_labels()
        elif num_classes > 2:
            # Multi-class classification
            if np.min(unique_labels) < 0:
                raise ValueError(f"Negative label values detected: {unique_labels}")

            # Check for sequential labels (starting from 0)
            if not np.array_equal(unique_labels, np.arange(num_classes)):
                # Transform labels for sequential numbering from 0
                self._transform_multiclass_labels()
        else:
            raise ValueError(f"Too few classes in data: {num_classes}")

    def _transform_binary_labels(self):
        """
        Transform labels for binary classification.
        """
        unique_labels = np.unique(np.concatenate([self.y_train, self.y_test]))
        print(f"Transforming binary classification labels: {unique_labels} -> [0, 1]")

        # Transform to 0 and 1
        label_map = {unique_labels[0]: 0, unique_labels[1]: 1}

        # Store for use in evaluation
        self.data_info['original_labels'] = unique_labels
        self.data_info['label_map'] = label_map

        # Transform labels
        self.y_train = np.array([label_map[label] for label in self.y_train])
        self.y_test = np.array([label_map[label] for label in self.y_test])

    def _transform_multiclass_labels(self):
        """
        Transform labels for multi-class classification.
        """
        unique_labels = np.unique(np.concatenate([self.y_train, self.y_test]))
        print(f"Transforming multi-class classification labels: {unique_labels} -> [0...{len(unique_labels) - 1}]")

        # Create label mapping
        label_map = {label: i for i, label in enumerate(unique_labels)}

        # Store for use in evaluation
        self.data_info['original_labels'] = unique_labels
        self.data_info['label_map'] = label_map
        self.data_info['num_classes'] = len(unique_labels)

        # Transform labels
        self.y_train = np.array([label_map[label] for label in self.y_train])
        self.y_test = np.array([label_map[label] for label in self.y_test])

    def _validate_regression_data(self):
        """
        Validate data for regression task.
        """
        if self.y_train is None or self.y_test is None:
            raise ValueError("Target values are required for regression")

        # Convert to numpy arrays
        if not isinstance(self.y_train, np.ndarray):
            self.y_train = np.array(self.y_train)
        if not isinstance(self.y_test, np.ndarray):
            self.y_test = np.array(self.y_test)

        # Check for invalid values
        if np.isnan(self.y_train).any() or np.isnan(self.y_test).any():
            raise ValueError("NaN values detected in target variables")

        # Store data information
        self.data_info['y_min'] = min(np.min(self.y_train), np.min(self.y_test))
        self.data_info['y_max'] = max(np.max(self.y_train), np.max(self.y_test))

    def _validate_time_series_data(self):
        """
        Validate data for time series forecasting task.
        """
        # Basic validation
        if self.X_train is None or self.X_test is None:
            raise ValueError("Time series data is missing")

        # Check data shape
        if isinstance(self.X_train, np.ndarray):
            if len(self.X_train.shape) != 3:
                raise ValueError(
                    f"Incorrect input data shape for time series. Expected 3D array, got: {self.X_train.shape}")
        elif isinstance(self.X_train, tf.keras.utils.Sequence):
            # For sequence generators, validation is performed during loading
            pass
        else:
            raise ValueError(f"Unsupported data type for time series: {type(self.X_train)}")

    def _store_data_info(self):
        """
        Store data information for use in other methods.
        """
        # Basic data information
        try:
            if isinstance(self.X_train, np.ndarray):
                self.data_info['x_shape'] = self.X_train.shape
            if isinstance(self.y_train, np.ndarray):
                self.data_info['y_shape'] = self.y_train.shape

            if self.task == TaskType.CLASSIFICATION:
                unique_train = np.unique(self.y_train)
                unique_test = np.unique(self.y_test)
                all_unique = np.unique(np.concatenate([unique_train, unique_test]))

                self.data_info['num_classes'] = len(all_unique)
                self.data_info['unique_labels'] = all_unique
                print(f"Detected {self.data_info['num_classes']} classes with labels: {all_unique}")

                # Check for missing classes in training data
                if len(unique_train) != len(all_unique):
                    print(f"Warning: Some classes are missing in training data. Training labels: {unique_train}")

            elif self.task == TaskType.REGRESSION:
                # Additional information for regression
                if isinstance(self.y_train, np.ndarray) and isinstance(self.y_test, np.ndarray):
                    self.data_info['y_mean'] = np.mean(np.concatenate([self.y_train, self.y_test]))
                    self.data_info['y_std'] = np.std(np.concatenate([self.y_train, self.y_test]))

        except Exception as e:
            print(f"Error collecting data information: {str(e)}")

    def _compile_model(self, model) -> None:
        """
        Compile Keras model with task-specific parameters.

        Args:
            model: Keras model to compile
        """
        # Default compilation parameters
        default_compile_params = {
            'optimizer': 'adam',
            'metrics': ['accuracy']
        }

        # Determine loss function based on task type and data characteristics
        if 'loss' not in self._params:
            if self.task == TaskType.CLASSIFICATION:
                # For classification, choice depends on number of classes and label format
                num_classes = self.data_info.get('num_classes', 0)

                # Check model's output layer
                try:
                    output_units = model.layers[-1].output_shape[-1]
                    if output_units != num_classes:
                        print(f"Warning: Number of neurons in output layer ({output_units}) "
                              f"doesn't match number of classes ({num_classes})")
                except:
                    output_units = None

                if num_classes == 2:
                    # For binary classification
                    default_compile_params['loss'] = 'binary_crossentropy'
                    # Ensure last layer has sigmoid activation
                    try:
                        last_layer_activation = model.layers[-1].activation.__name__
                        if last_layer_activation != 'sigmoid':
                            print(f"Warning: For binary classification, sigmoid activation is recommended "
                                  f"in output layer, not {last_layer_activation}")
                    except:
                        pass
                else:
                    # For multi-class classification
                    default_compile_params['loss'] = 'sparse_categorical_crossentropy'
                    # Ensure last layer has softmax activation
                    try:
                        last_layer_activation = model.layers[-1].activation.__name__
                        if last_layer_activation != 'softmax':
                            print(f"Warning: For multi-class classification, softmax activation is recommended "
                                  f"in output layer, not {last_layer_activation}")
                    except:
                        pass
            elif self.task == TaskType.REGRESSION:
                default_compile_params['loss'] = 'mse'
                default_compile_params['metrics'] = ['mae', 'mse']
            elif self.task == TaskType.ANOMALY_DETECTION:
                default_compile_params['loss'] = 'binary_crossentropy'
            elif self.task == TaskType.TIME_SERIES_FORECASTING:
                default_compile_params['loss'] = 'mse'
                default_compile_params['metrics'] = ['mae']
            else:
                default_compile_params['loss'] = 'mse'  # Default

        # Update default parameters with passed parameters
        compile_params = {**default_compile_params}

        # Update with model parameters in self._params
        if 'model_params' in self._params:
            model_params = self._params.get('model_params', {})
            for key in ['loss', 'metrics']:
                if key in model_params and model_params[key] is not None:
                    compile_params[key] = model_params[key]

            # Handle optimizer with learning_rate consideration
            if 'optimizer' in model_params and model_params['optimizer'] is not None:
                optimizer_name = model_params['optimizer']
                learning_rate = model_params.get('learning_rate')

                # Create optimizer object with specified learning_rate
                if learning_rate is not None:
                    if optimizer_name == 'adam':
                        compile_params['optimizer'] = tf.keras.optimizers.Adam(learning_rate=learning_rate)
                    elif optimizer_name == 'sgd':
                        compile_params['optimizer'] = tf.keras.optimizers.SGD(learning_rate=learning_rate)
                    elif optimizer_name == 'rmsprop':
                        compile_params['optimizer'] = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)
                    elif optimizer_name == 'adagrad':
                        compile_params['optimizer'] = tf.keras.optimizers.Adagrad(learning_rate=learning_rate)
                    elif optimizer_name == 'adadelta':
                        compile_params['optimizer'] = tf.keras.optimizers.Adadelta(learning_rate=learning_rate)
                    elif optimizer_name == 'adamax':
                        compile_params['optimizer'] = tf.keras.optimizers.Adamax(learning_rate=learning_rate)
                    elif optimizer_name == 'nadam':
                        compile_params['optimizer'] = tf.keras.optimizers.Nadam(learning_rate=learning_rate)
                    else:
                        # If optimizer name is not recognized, use simple string name
                        compile_params['optimizer'] = optimizer_name
                        print(
                            f"Warning: Unrecognized optimizer '{optimizer_name}'. Learning rate will be ignored.")
                else:
                    # If learning_rate is not specified, use just the optimizer name
                    compile_params['optimizer'] = optimizer_name

        # Compile model
        try:
            model.compile(**compile_params)
            print(f"Model compiled with parameters: {compile_params}")
        except Exception as e:
            raise ValueError(f"Error compiling model: {str(e)}")

    def _train_model(self, model):
        """
        Train Keras model with task-specific parameters.

        Args:
            model: Keras model to train

        Returns:
            History: Training history
        """
        # Default training parameters
        default_fit_params = {
            'x': self.X_train,
            'batch_size': 32,
            'epochs': 10,
            'verbose': 1,
            'validation_split': 0.2,
            'shuffle': True,
            'y': self.y_train
        }

        # Add target variables depending on task type
        if self.task in [TaskType.CLASSIFICATION, TaskType.REGRESSION]:
            default_fit_params['y'] = self.y_train

        # For tasks with generators
        if isinstance(self.X_train, tf.keras.utils.Sequence):
            # For generators, no need to specify y
            default_fit_params = {
                'x': self.X_train,
                'batch_size': None,  # Generator determines batch size itself
                'epochs': 10,
                'verbose': 1,
                'validation_data': self.X_test,
                'shuffle': False  # Generator can shuffle data itself
            }
            if 'y' in default_fit_params:
                del default_fit_params['y']
            if 'validation_split' in default_fit_params:
                del default_fit_params['validation_split']

        # Update training parameters
        fit_params = {**default_fit_params}

        # Update with training parameters in self._params
        if 'fit_params' in self._params:
            for key, value in self._params.get('fit_params', {}).items():
                if value is not None:
                    fit_params[key] = value

        # Remove None parameters
        for key in list(fit_params.keys()):
            if fit_params[key] is None:
                del fit_params[key]

        # Check for empty class_weight dictionary
        if 'class_weight' in fit_params and fit_params['class_weight'] == {}:
            del fit_params['class_weight']

        # Add balanced class weights for imbalanced classification data
        if self.task == TaskType.CLASSIFICATION and 'class_weight' not in fit_params:
            try:
                unique_train_labels, counts = np.unique(self.y_train, return_counts=True)
                if len(unique_train_labels) > 1 and np.max(counts) / np.min(counts) > 5:
                    print("Detected imbalanced dataset. Applying automatic class weights.")
                    class_weights = {}
                    for i, count in enumerate(counts):
                        class_weights[i] = len(self.y_train) / (len(unique_train_labels) * count)
                    fit_params['class_weight'] = class_weights
            except Exception as e:
                print(f"Failed to automatically determine class weights: {str(e)}")

        if 'x' in fit_params and not isinstance(fit_params['x'], tf.keras.utils.Sequence):
            fit_params['x'] = self._convert_to_tensorflow_compatible(fit_params['x'])
        if 'y' in fit_params and not isinstance(fit_params['y'], tf.keras.utils.Sequence):
            fit_params['y'] = self._convert_to_tensorflow_compatible(fit_params['y'])
        if 'validation_data' in fit_params and not isinstance(fit_params['validation_data'], tf.keras.utils.Sequence):
            if isinstance(fit_params['validation_data'], tuple):
                # If validation_data is a tuple (x_val, y_val)
                x_val, y_val = fit_params['validation_data']
                x_val = self._convert_to_tensorflow_compatible(x_val)
                y_val = self._convert_to_tensorflow_compatible(y_val)
                fit_params['validation_data'] = (x_val, y_val)
            else:
                fit_params['validation_data'] = self._convert_to_tensorflow_compatible(fit_params['validation_data'])

        # Train model
        try:
            history = model.fit(**fit_params)
            return history
        except Exception as e:
            error_msg = str(e)
            if "SparseSoftmaxCrossEntropyWithLogits" in error_msg and "valid range" in error_msg:
                # Detailed error message about class labels
                unique_labels = np.unique(self.y_train)

                error_details = (
                    f"Помилка при навчанні моделі класифікації:\n"
                    f"Виявлено мітки: {unique_labels}\n"
                    # f"Оригінальна помилка: {error_msg}"
                )
                raise ValueError(error_details)
            else:
                raise

    def evaluate(self) -> None:
        """
        Simplified evaluation of results - gets metric strategy for given network and task,
        and calls it with appropriate parameters
        """
        if not self.is_finished:
            raise RuntimeError("Experiment must be completed before evaluation")

        # Initialize metric dictionaries
        self.train_metrics = {}
        self.test_metrics = {}

        try:
            # Get metric strategy for given model and task combination
            self.metric_strategy = get_nn_metric(NNModelType.GENERIC.value, self.task.value)

            # Determine input data for evaluation depending on task type
            if self.task in [TaskType.CLASSIFICATION, TaskType.REGRESSION, TaskType.TIME_SERIES_FORECASTING]:
                # For classification, regression and forecasting - use actual and predictions
                train_input = (self.train_actual, self.train_predictions)
                test_input = (self.test_actual, self.test_predictions)
            else:
                # Universal approach for other tasks
                train_input = (self.X_train, self.train_predictions) if hasattr(self, 'train_predictions') else None
                test_input = (self.X_test, self.test_predictions) if hasattr(self, 'test_predictions') else None

            # Perform evaluation for training data if available
            if train_input and all(x is not None for x in train_input):
                self.train_metrics.update(
                    self.metric_strategy.evaluate(*train_input)
                )

            # Perform evaluation for test data if available
            if test_input and all(x is not None for x in test_input):
                self.test_metrics.update(
                    self.metric_strategy.evaluate(*test_input)
                )

        except Exception as e:
            print(f"Error during evaluation: {str(e)}")
            self.train_metrics = {"error": f"Evaluation error: {str(e)}"}
            self.test_metrics = {"error": f"Evaluation error: {str(e)}"}

        # Signal evaluation completion
        self.experiment_evaluated.emit(self.train_metrics, self.test_metrics)

    def _convert_probabilities_to_classes(self, predictions, threshold=0.5):
        if predictions is None:
            return None

        # Check prediction shape
        if not isinstance(predictions, np.ndarray):
            try:
                predictions = np.array(predictions)
            except Exception as e:
                raise ValueError(f"{str(e)}")

        # For binary classification with single output (sigmoid)
        if len(predictions.shape) == 1 or (len(predictions.shape) == 2 and predictions.shape[1] == 1):
            return (predictions > threshold).astype(int)

        # For multi-class classification (softmax)
        elif len(predictions.shape) == 2 and predictions.shape[1] > 1:
            return np.argmax(predictions, axis=1)

        # For other prediction formats
        else:
            raise ValueError(f"{predictions.shape}")

    def _load_data(self):
        """
        Load data depending on neural network type and task.
        Extended method that handles different data types: tabular, images,
        sequences, text and specialized data for autoencoders.
        """
        super()._load_data()
        print(
            f"Data loaded for generic model: X_train={self.X_train.shape if hasattr(self.X_train, 'shape') else 'N/A'}")

        # Check data after loading
        self._check_loaded_data()

    def _check_loaded_data(self):
        """
        Check correctness of loaded data before use.
        """
        # Check data availability
        if self.X_train is None:
            raise ValueError("Training data not loaded (X_train is None)")
        # Check X and y correspondence
        if self.task in [TaskType.CLASSIFICATION, TaskType.REGRESSION]:
            if self.y_train is None:
                raise ValueError("Training target variables not loaded (y_train is None)")

            if isinstance(self.X_train, np.ndarray) and isinstance(self.y_train, np.ndarray):
                if len(self.X_train) != len(self.y_train):
                    raise ValueError(
                        f"X_train ({len(self.X_train)}) and y_train ({len(self.y_train)}) dimensions don't match")

                if len(self.X_test) != len(self.y_test):
                    raise ValueError(
                        f"X_test ({len(self.X_test)}) and y_test ({len(self.y_test)}) dimensions don't match")

    def _convert_to_tensorflow_compatible(self, data):

        if data is None:
            return None

        if isinstance(data, tf.Tensor):
            return data

        # If data is pandas DataFrame or Series
        if hasattr(data, 'values'):
            data = data.values

        # If data is iterable object
        if not isinstance(data, np.ndarray):
            try:
                data = np.array(data)
            except Exception as e:
                raise ValueError(f"Failed to convert data type {type(data)} to numpy array: {str(e)}")

        # Convert data to appropriate type
        if data.dtype == np.int32:
            data = data.astype(np.int32)
        elif data.dtype == np.float32:
            data = data.astype(np.float32)
        elif data.dtype == bool:
            data = data.astype(np.bool_)
        elif np.issubdtype(data.dtype, np.object_):
            # Find appropriate data type for array
            if all(isinstance(x, (int, np.integer)) for x in data.flatten() if x is not None):
                data = data.astype(np.int32)
            elif all(isinstance(x, (float, np.floating)) for x in data.flatten() if x is not None):
                data = data.astype(np.float32)
            elif all(isinstance(x, str) for x in data.flatten() if x is not None):
                pass
            else:
                try:
                    data = data.astype(np.float32)
                except Exception as e:
                    raise ValueError(f"Failed to convert array to numeric format: {str(e)}")

        try:
            tensor = tf.convert_to_tensor(data)
            return tensor
        except Exception as e:
            raise ValueError(
                f"Error converting to TensorFlow tensor: {str(e)}\nData type: {data.dtype}, Shape: {data.shape}")

файл input_data_params.py
from project.logic.evaluation.task_register import TaskType
from project.logic.modules import task_names


class InputDataParams:

    def __init__(self):
        self.mode = 'single_file'

        self.single_file_path = ''
        self.x_train_file_path = ''
        self.y_train_file_path = ''
        self.x_test_file_path = ''
        self.y_test_file_path = ''

        self.train_percent = 80
        self.test_percent = 20
        self.seed = 42

        self.target_variable = ''

        self.file_encoding = 'utf-8'

        self.file_separator = ','

        self.current_task = ''

        self.categorical_encoding = 'one-hot'

    def to_dict(self):
        data = {
            'mode': self.mode,
            'current_task': self.current_task
        }

        if self.mode == 'single_file':
            data.update({
                'single_file_path': self.single_file_path,
                'train_percent': self.train_percent,
                'test_percent': self.test_percent,
                'seed': self.seed,
                'file_encoding': self.file_encoding,
                'file_separator': self.file_separator,
                'categorical_encoding': self.categorical_encoding
            })

            if self.target_variable and not self.is_target_not_required():
                data['target_variable'] = self.target_variable
        else:
            if self.is_target_not_required():
                data.update({
                    'x_train_file_path': self.x_train_file_path,
                    'x_test_file_path': self.x_test_file_path,
                    'file_encoding': self.file_encoding,
                    'file_separator': self.file_separator,
                    'categorical_encoding': self.categorical_encoding
                })
            else:
                data.update({
                    'x_train_file_path': self.x_train_file_path,
                    'y_train_file_path': self.y_train_file_path,
                    'x_test_file_path': self.x_test_file_path,
                    'y_test_file_path': self.y_test_file_path,
                    'file_encoding': self.file_encoding,
                    'file_separator': self.file_separator,
                    'categorical_encoding': self.categorical_encoding
                })

        return data

    def is_target_not_required(self):
        return self.current_task in InputDataParams.tasks_without_target

    def is_filled(self):
        if self.mode == "single_file":
            if self.single_file_path != "":
                return True
        else:
            if self.x_test_file_path and self.x_train_file_path and self.y_train_file_path and self.y_test_file_path != "":
                return True
        return False

    tasks_without_target = [
        task_names.CLUSTERING,
        task_names.DIMENSIONALITY_REDUCTION,
        task_names.ANOMALY_DETECTION,
        task_names.DENSITY_ESTIMATION,
        TaskType.DIMENSIONALITY_REDUCTION,
        TaskType.ANOMALY_DETECTION
    ]


файл nn_experiment.py
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, TypeVar

import numpy as np
import tensorflow as tf
from enum import Enum
from project.logic.evaluation.task_register import TaskType
from project.logic.experiment.experiment import Experiment

T = TypeVar('T', bound='NeuralNetworkExperiment')


class NeuralNetworkExperiment(Experiment):
    """
    Abstract base class for neural network experiments that defines a common interface
    for all neural network experiment implementations.
    """

    def __init__(self, id: int, task: TaskType, model: Any, params: Dict[str, Any], parent=None,load_type="", model_file="",
                 weights_file=""):
        """
        Initialize the neural network experiment.

        Args:
            id: Experiment ID
            task: Task type (from TaskType enum)
            model: Neural network model
            params: Dictionary of parameters
            parent: Parent experiment (if any)
        """
        super().__init__(id, task, model, params, parent)

        # Common attributes for all neural network experiments
        self.history: Optional[tf.keras.callbacks.History] = None
        self.model_file_path: str = model_file
        self.weights_file_path: str = weights_file
        self.load_type: str = load_type

        # Data attributes
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None

        # Prediction results
        self.train_predictions = None
        self.test_predictions = None
        self.train_actual = None
        self.test_actual = None

        # Training information
        self.trained_model = None
        self.train_time = 0.0
        self.is_finished = False

        # Evaluation metrics
        self.train_metrics = {}
        self.test_metrics = {}

        # Data info
        self.data_info = {}

    @abstractmethod
    def run(self) -> None:
        """Run the neural network experiment."""
        pass

    @abstractmethod
    def evaluate(self) -> None:
        """Evaluate the results of the neural network experiment."""
        pass

    @abstractmethod
    def _compile_model(self, model) -> None:
        """Compile the neural network model."""
        pass

    @abstractmethod
    def _train_model(self, model) -> Any:
        """Train the neural network model."""
        pass
    def load_model_from_file(self) -> None:
        """
        Завантажити модель з файлу.
        """
        try:
            if not self.model_file_path:
                raise ValueError("Шлях до файлу моделі не вказано")

            if self.load_type == 'Keras (.h5)':
                self.model = tf.keras.models.load_model(self.model_file_path, compile=False)
            elif self.load_type == 'TensorFlow SavedModel':
                self.model = tf.keras.models.load_model(self.model_file_path)
            elif self.load_type == 'JSON + Weights':
                with open(self.model_file_path, 'r') as json_file:
                    model_json = json_file.read()
                self.model = tf.keras.models.model_from_json(model_json)
                if self.weights_file_path:
                    self.model.load_weights(self.weights_file_path)
            else:
                raise ValueError(f"Непідтримуваний тип моделі: {self.load_type}")

        except Exception as e:
            raise ValueError(f"Помилка при завантаженні моделі: {str(e)}")
    @abstractmethod
    def _validate_data(self) -> None:
        """Validate the input data for the experiment."""
        pass

    # Common utility methods that can be implemented here
    def _convert_to_tensorflow_compatible(self, data):
        """
        Convert data to a TensorFlow-compatible format.

        Args:
            data: Input data to convert

        Returns:
            Data in TensorFlow-compatible format
        """
        if data is None:
            return None

        if isinstance(data, tf.Tensor):
            return data

        # If data is pandas DataFrame or Series
        if hasattr(data, 'values'):
            data = data.values

        # If data is iterable but not numpy array
        if not isinstance(data, np.ndarray):
            try:
                data = np.array(data)
            except Exception as e:
                raise ValueError(f"Failed to convert data of type {type(data)} to numpy array: {str(e)}")

        # Convert data to appropriate type
        if data.dtype == np.int32:
            data = data.astype(np.int32)
        elif data.dtype == np.float32:
            data = data.astype(np.float32)
        elif data.dtype == bool:
            data = data.astype(np.bool_)
        elif np.issubdtype(data.dtype, np.object_):
            # Find appropriate data type for array
            if all(isinstance(x, (int, np.integer)) for x in data.flatten() if x is not None):
                data = data.astype(np.int32)
            elif all(isinstance(x, (float, np.floating)) for x in data.flatten() if x is not None):
                data = data.astype(np.float32)
            elif all(isinstance(x, str) for x in data.flatten() if x is not None):
                pass
            else:
                try:
                    data = data.astype(np.float32)
                except Exception as e:
                    raise ValueError(f"Failed to convert array to numeric format: {str(e)}")

        try:
            tensor = tf.convert_to_tensor(data)
            return tensor
        except Exception as e:
            raise ValueError(
                f"Error converting to TensorFlow tensor: {str(e)}\nData type: {data.dtype}, Shape: {data.shape}")

файл nn_input_data_params.py
from typing import Dict, Any

from project.logic.experiment.input_data_params import InputDataParams


class NeuralNetInputDataParams(InputDataParams):

    def __init__(self):
        super().__init__()

        # Шляхи до файлів моделі
        self.model_file_path = ''
        self.weights_file_path = ''
        self.model_config_path = ''

        # Додаткові параметри для нейронних мереж
        self.load_type = ''

        self.text_directory = ''
        self.image_directory = ''

    def to_dict(self) -> Dict[str, Any]:
        data = super().to_dict()
        data.update({
            'model_file_path': self.model_file_path,
            'weights_file_path': self.weights_file_path,
            'model_config_path': self.model_config_path,
            'load_type': self.load_type,
        })

        return data

файл basic_contoller.py
from PyQt5.QtWidgets import QApplication



class BasicController:
    def __init__(self, view):
        self.isMaximized = True
        self.view = view
        self.close_arrow = self.view.style().standardIcon(QApplication.style().SP_ArrowLeft)
        self.open_arrow = self.view.style().standardIcon(QApplication.style().SP_ArrowRight)



файл inspector_controller.py
from PyQt5.QtCore import Qt
from PyQt5.QtWidgets import QListWidgetItem, QWidget, QListWidget

from project.controllers.basic_contoller import BasicController
from project.controllers.node_controller import NodeController


class InspectorController(BasicController):
    """Node inspector - displays a list of nodes and allows managing them."""

    def __init__(self, view, scene, node_view):
        super().__init__(view)
        self.node_controller = NodeController(scene, node_view)
        self.nodes_list = self.view.findChild(QListWidget, "nodes_list")

        # Connecting to the node controller
        self.node_controller.node_created.connect(self.add_node_to_inspector)
        self.node_controller.node_deleted.connect(self.remove_node_from_inspector)
        self.node_controller.node_renamed.connect(self.update_node_in_inspector)
        self.nodes_list.itemClicked.connect(self.on_item_clicked)

    def add_node_to_inspector(self, node):
        """Adds a node to the inspector."""
        item = QListWidgetItem(node.get_name())
        item.setData(Qt.UserRole, node.id)  # Storing the node ID
        self.nodes_list.addItem(item)

    def remove_node_from_inspector(self, node_id):
        """Removes a node from the inspector by ID."""
        for i in range(self.nodes_list.count()):
            item = self.nodes_list.item(i)
            if item.data(Qt.UserRole) == node_id:
                self.nodes_list.takeItem(i)
                break

    def update_node_in_inspector(self, node):
        """Updates the node display in the inspector."""
        for i in range(self.nodes_list.count()):
            item = self.nodes_list.item(i)
            if item.data(Qt.UserRole) == node.id:
                item.setText(node.get_name())
                self.node_controller.update_experiment_name.emit(node.id, node.get_name())
                break

    def on_item_clicked(self, item):
        """Click handler for list items."""
        node_id = item.data(Qt.UserRole)
        self.node_controller.center_on_node(node_id)

файл main_controller.py
from PyQt5.QtWidgets import QMainWindow, QDialog

from project.controllers.experiment_settings_dialog.experiment_settings_controller import ExperimentSettingsController
from project.controllers.inspector_controller import InspectorController
from project.controllers.task_selector_controller import TaskSelectorController
from project.controllers.workspace_manager import WorkspaceManager
from project.logic.experiment_manager import ExperimentManager
from project.logic.modules.models_manager import ModelsManager
from project.ui.experiment_settings_dialog.experiment_comparison_dialog import ExperimentComparisonDialog
from project.ui.experiment_settings_dialog.experiment_settings_dialog import ExperimentSettingsWindow

from project.ui.main_window import MainWindow


class MainController:
    def __init__(self):
        self.workspace_manager = WorkspaceManager()
        self.view = MainWindow()
        self.models_manager = ModelsManager()
        self.inspector_controller = InspectorController(self.view.inspector_frame,
                                                        self.view.scene,
                                                        self.view.graphics_view)
        self.task_selector_controller = TaskSelectorController(self.view)
        self.experiment_manager = ExperimentManager()

        self.workspace_manager.set_experiment_manager(self.experiment_manager)
        self.workspace_manager.set_node_controller(self.inspector_controller.node_controller)
        self.workspace_manager.set_work_area(self.view.graphics_view)  # Passing the work area

        self.connect_signals()

    def connect_signals(self):
        self.task_selector_controller.request_models_dict.connect(self.models_manager.create_models_dict)
        self.models_manager.models_dict_ready.connect(self.task_selector_controller.handle_models_dict_response)
        self.view.signals.add_new_experiment.connect(self.inspector_controller.node_controller.create_node)
        self.inspector_controller.node_controller.node_created.connect(self.experiment_manager.get_node)
        self.view.signals.add_new_experiment.connect(self.task_selector_controller.show_approach_selection)
        self.task_selector_controller.send_ml_model.connect(self.experiment_manager.get_ml_model)
        self.inspector_controller.node_controller.nodeInfoOpened.connect(self._show_experiment_settings_dialog)

        # Connecting the experiment inheritance signal from the node controller
        self.inspector_controller.node_controller.experiment_inherited.connect(self._handle_experiment_inheritance)

        self.task_selector_controller.send_nn_model.connect(self.experiment_manager.create_nn_experiment)

        self.view.fit_action.triggered.connect(self.workspace_manager.fit_view_to_content)
        self.view.save_as_action.triggered.connect(lambda: self.workspace_manager.save_project_as(self.view))
        self.view.save_action.triggered.connect(lambda: self.workspace_manager.save_project(self.view))
        self.view.open_action.triggered.connect(lambda: self.workspace_manager.open_project(self.view))
        self.view.new_action.triggered.connect(self.workspace_manager.new_project)

        self.inspector_controller.node_controller.update_experiment_name.connect(self.experiment_manager.update_name)


    def _show_experiment_settings_dialog(self, node_id):
        """Function to display the experiment settings dialog"""
        dialog = ExperimentSettingsWindow(self.view)
        experiment = self.experiment_manager.get_experiment(node_id)
        self.experiment_settings_controller = ExperimentSettingsController(experiment, dialog)

        # Connecting the inheritance signal from settings dialog
        self.experiment_settings_controller.experiment_inherited.connect(self._handle_experiment_inheritance)
        self.experiment_settings_controller.metrics_controller.compare_experiments.connect(
            self.experiment_manager.show_comparison_dialog)

        #self.experiment_settings_controller.request_experiment_update.connect(
        #    self.experiment_manager.update_nn_experiment)  # change nn experiment class in order to task

        self.experiment_settings_controller.show()
        self.experiment_settings_controller.window.general_tab.save_button.clicked.connect(
            lambda: self.experiment_manager.save_model(experiment, self.view))

        self.experiment_settings_controller.metrics_controller.compare_all.connect(self.experiment_manager.get_experiments_by_task)
        self.experiment_manager.get_all_task_experiments.connect(ExperimentComparisonDialog.create_dialog_with_filtered_experiments)

    def _handle_experiment_inheritance(self, parent_id):
        """Handler for experiment inheritance signal"""
        # Create new node and inherit experiment data
        parent_experiment = self.experiment_manager.get_experiment(parent_id)

        # Create new node through node controller
        new_node = self.inspector_controller.node_controller.create_inherited_node(parent_id)

        # After node creation, inherit the experiment
        if new_node and parent_experiment:
            # Experiment manager will create new experiment with inherited data
            self.experiment_manager.inherit_experiment_from(parent_id, new_node.id)

    def show(self):
        self.view.show()


файл node_controller.py
from PyQt5.QtCore import Qt, QPoint, QTimer
from PyQt5.QtWidgets import QGraphicsView, QMenu, QAction, QMessageBox, QGraphicsItem

from project.ui.Edge import Edge
from project.ui.node import Node
from PyQt5.QtCore import pyqtSignal, QObject


class NodeController:
    """Node controller - responsible for creating, managing and interacting with nodes."""

    def __init__(self, scene, view):
        self.scene = scene
        self.view = view
        self.nodes = []
        self.edges = []  # Adding a list to store connections
        self.active_node = None
        self.drag_timer = QTimer()
        self.drag_timer.setSingleShot(True)
        self.drag_timer.timeout.connect(self._start_dragging)
        self.current_press_node = None

        # Create a mediator class for signals
        class SignalEmitter(QObject):
            node_created = pyqtSignal(object)  # Node creation signal
            node_deleted = pyqtSignal(int)  # Node deletion signal (passes ID)
            node_renamed = pyqtSignal(object)  # Node rename signal
            update_experiment_name = pyqtSignal(int, str)
            nodeInfoOpened = pyqtSignal(int)  # Signal for opening node info
            experiment_inherited = pyqtSignal(int)  # Experiment inheritance signal

        self.signals = SignalEmitter()
        self.node_created = self.signals.node_created
        self.node_deleted = self.signals.node_deleted
        self.node_renamed = self.signals.node_renamed
        self.nodeInfoOpened = self.signals.nodeInfoOpened
        self.experiment_inherited = self.signals.experiment_inherited
        self.update_experiment_name = self.signals.update_experiment_name

        self.scene.installEventFilter(view)
        view.mousePressEvent = self._view_mouse_press
        view.mouseReleaseEvent = self._view_mouse_release
        view.mouseMoveEvent = self._view_mouse_move
        view.contextMenuEvent = self._view_context_menu

    def create_node(self, x=None, y=None):
        """Creates a new node at specified coordinates or at the center of visible area."""
        node = Node()

        if x is None or y is None:
            # Get the visible viewport area
            viewport_rect = self.view.viewport().rect()

            # Convert viewport center to scene coordinates
            viewport_center = QPoint(viewport_rect.width() // 2, viewport_rect.height() // 2)
            scene_center = self.view.mapToScene(viewport_center)

            # Account for node size for precise centering
            node_width = node.boundingRect().width()
            node_height = node.boundingRect().height()

            # Set node position at viewport center
            node.setPos(scene_center.x() - node_width / 2, scene_center.y() - node_height / 2)
        else:
            # Set node position at specified coordinates
            node.setPos(x, y)

        self.scene.addItem(node)
        self.nodes.append(node)

        self.node_created.emit(node)

        return node

    def create_inherited_node(self, parent_node_id):
        """Creates a new node that inherits from the node with specified ID."""
        # Find parent node by ID
        parent_node = self.find_node_by_id(parent_node_id)

        if not parent_node:
            print(f"Error: node with ID {parent_node_id} not found")
            return None

        # Determine position for new node (slightly to the right and below parent)
        parent_pos = parent_node.pos()
        new_x = parent_pos.x() + 150  # Right offset
        new_y = parent_pos.y() + 100  # Down offset

        # Create new node
        new_node = self.create_node(new_x, new_y)

        # Set name indicating inheritance
        new_node.set_name(f"Успадковано від {parent_node.get_name()}")

        # Create connection between nodes
        self.create_edge(parent_node, new_node)

        return new_node

    def create_edge(self, source_node, target_node):
        """Creates a connection between two nodes."""
        edge = Edge(source_node, target_node)
        self.scene.addItem(edge)
        self.edges.append(edge)

        # Connect edge update when nodes move
        source_node.itemChange = self._wrap_item_change(source_node, edge)
        target_node.itemChange = self._wrap_item_change(target_node, edge)

        return edge

    def _wrap_item_change(self, node, edge):
        """Creates a wrapper for node's itemChange method to update the connection."""
        original_item_change = node.itemChange if hasattr(node, 'itemChange') else lambda change, value: value

        def wrapped_item_change(change, value):
            result = original_item_change(change, value)
            if change == QGraphicsItem.ItemPositionChange or change == QGraphicsItem.ItemPositionHasChanged:
                edge.update_position()
            return result

        return wrapped_item_change

    def find_node_by_id(self, node_id):
        """Finds a node by its ID."""
        for node in self.nodes:
            if node.id == node_id:
                return node
        return None

    def delete_node(self, node):
        """Deletes a node and all its connections."""
        if node in self.nodes:
            node_id = node.id

            # Remove all connections related to this node
            edges_to_remove = [edge for edge in self.edges
                               if edge.source_node == node or edge.target_node == node]

            for edge in edges_to_remove:
                self.scene.removeItem(edge)
                self.edges.remove(edge)

            # Remove the node itself
            self.scene.removeItem(node)
            self.nodes.remove(node)

            if self.active_node == node:
                self.active_node = None

            # Emit node deletion signal
            self.node_deleted.emit(node_id)

    def open_node_info(self, node):
        """Opens a dialog with node information."""
        self.nodeInfoOpened.emit(node.id)

    def edit_node_name(self, node):
        """Activates node name editing."""
        node.start_editing_name()
        # Subscribe to editing finished signal
        node.name_editor.editingFinished.connect(lambda: self.node_renamed.emit(node))

    def center_on_node(self, node_id):
        """Centers view on the node with specified ID."""
        for node in self.nodes:
            if node.id == node_id:
                # Get node center
                node_center = node.mapToScene(
                    node.boundingRect().center().x(),
                    node.boundingRect().center().y()
                )
                # Center view on node
                self.view.centerOn(node_center)
                break

    def _view_mouse_press(self, event):
        """Mouse press event handler for GraphicsView."""
        # Get item under mouse
        pos = self.view.mapToScene(event.pos())
        item = self.scene.itemAt(pos, self.view.transform())

        if event.button() == Qt.LeftButton and isinstance(item, Node):
            self.current_press_node = item
            # Start timer to detect click-and-hold
            self.drag_timer.start(200)
        else:
            # Standard processing for non-node items
            QGraphicsView.mousePressEvent(self.view, event)

    def _view_mouse_release(self, event):
        """Mouse release event handler for GraphicsView."""
        if event.button() == Qt.LeftButton and self.current_press_node:
            if self.drag_timer.isActive():
                # If timer is still active, it's a click
                self.drag_timer.stop()
                self.open_node_info(self.current_press_node)
            else:
                # If timer is inactive, it's drag end
                if self.active_node:
                    self.active_node.set_active(False)
                    self.active_node = None

            self.current_press_node = None
        else:
            # Standard processing for other cases
            QGraphicsView.mouseReleaseEvent(self.view, event)

    def _view_mouse_move(self, event):
        """Mouse move event handler for GraphicsView."""
        if self.active_node:
            # If there's an active node, update its position
            pos = self.view.mapToScene(event.pos())
            node_width = self.active_node.boundingRect().width()
            node_height = self.active_node.boundingRect().height()
            self.active_node.setPos(pos.x() - node_width / 2, pos.y() - node_height / 2)
        else:
            # Standard processing for other cases
            QGraphicsView.mouseMoveEvent(self.view, event)

    def _view_context_menu(self, event):
        """Context menu event handler."""
        # Get item under mouse
        pos = self.view.mapToScene(event.pos())
        item = self.scene.itemAt(pos, self.view.transform())

        if isinstance(item, Node):
            # Create context menu
            context_menu = QMenu(self.view)

            # Add actions
            rename_action = QAction("Перейменувати", self.view)
            delete_action = QAction("Видалити", self.view)

            # Connect handlers
            rename_action.triggered.connect(lambda: self.edit_node_name(item))
            delete_action.triggered.connect(lambda: self.delete_node(item))

            # Add actions to menu
            context_menu.addAction(rename_action)
            context_menu.addAction(delete_action)

            # Show menu
            context_menu.exec_(event.globalPos())
        else:
            # Standard processing for other cases
            QGraphicsView.contextMenuEvent(self.view, event)

    def _start_dragging(self):
        """Activates node dragging mode."""
        if self.current_press_node:
            self.active_node = self.current_press_node
            self.active_node.set_active(True)

файл parameter_editor_controller.py
class ParameterEditorController:
    """Controller for ParameterEditorWidget"""

    def __init__(self):
        self.view = None

    def set_view(self, view):
        self.view = view

    def show(self, params_dict):
        """Loads parameters into the widget"""
        if self.view:
            self.view.populate_table(params_dict)

    def on_parameters_changed(self, updated_params):
        """Handler for parameter change event"""
        # Additional logic can be added here before further processing
        print("Controller: Parameters updated:", updated_params)
        return updated_params

    def get_current_parameters(self):
        """Returns current parameters from the widget"""
        if self.view:
            return self.view.get_current_parameters()
        return {}

файл serializer.py
import os
import pickle
from typing import Dict
from PyQt5.QtCore import QObject, pyqtSignal, QPointF, pyqtSlot
from project.logic.experiment.experiment import Experiment
from project.logic.experiment.generic_nn_experiment import GenericNeuralNetworkExperiment
from project.logic.experiment_manager import ExperimentManager

class WorkspaceSerializer(QObject):
    """
    Class for serializing and deserializing the experiment workspace.
    Stores all information about experiments, nodes and connections between them.
    """

    # Signals for operation status notifications
    workspace_saved = pyqtSignal(str)  # Signal emitted when workspace is saved (path)
    workspace_loaded = pyqtSignal(str)  # Signal emitted when workspace is loaded (path)
    save_error = pyqtSignal(str)  # Signal emitted when saving fails (error message)
    load_error = pyqtSignal(str)  # Signal emitted when loading fails (error message)

    def __init__(self, experiment_manager=None, node_controller=None):
        super().__init__()
        # References to experiment manager and node controller
        self.experiment_manager = experiment_manager or ExperimentManager()
        self.node_controller = node_controller

        # File format version (for future compatibility)
        self.file_format_version = "1.0"

    def set_node_controller(self, node_controller):
        """Sets the node controller for the serializer."""
        self.node_controller = node_controller

    def set_experiment_manager(self, experiment_manager):
        """Sets the experiment manager for the serializer."""
        self.experiment_manager = experiment_manager

    def save_workspace(self, filepath: str) -> bool:
        """
        Saves the workspace to a file.

        Args:
            filepath: Path to the save file

        Returns:
            bool: True on success, False on error
        """
        try:
            # Check if required dependencies are set
            if not self.experiment_manager or not self.node_controller:
                self.save_error.emit("Required dependencies not set")
                return False

            # Get data for serialization
            serialized_data = self._prepare_serialization_data()

            # Save data to file
            with open(filepath, 'wb') as file:
                pickle.dump(serialized_data, file, protocol=pickle.HIGHEST_PROTOCOL)

            # Notify about successful save
            self.workspace_saved.emit(filepath)
            return True

        except Exception as e:
            error_message = f"Error saving workspace: {str(e)}"
            print(error_message)
            self.save_error.emit(error_message)
            return False

    def load_workspace(self, filepath: str) -> bool:
        """
        Loads workspace from file.

        Args:
            filepath: Path to the file to load

        Returns:
            bool: True on success, False on error
        """
        try:
            # Check if required dependencies are set
            if not self.experiment_manager or not self.node_controller:
                self.load_error.emit("Required dependencies not set")
                return False

            # Check if file exists
            if not os.path.exists(filepath):
                self.load_error.emit(f"File {filepath} does not exist")
                return False

            # Load data from file
            with open(filepath, 'rb') as file:
                loaded_data = pickle.load(file)

            # Restore data
            self._restore_from_serialization_data(loaded_data)

            # Notify about successful load
            self.workspace_loaded.emit(filepath)
            return True

        except Exception as e:
            error_message = f"Error loading workspace: {str(e)}"
            print(error_message)
            self.load_error.emit(error_message)
            return False

    def _prepare_serialization_data(self) -> Dict:
        """
        Prepares data for serialization.

        Returns:
            Dict: Dictionary with serialization data
        """
        # Main dictionary for all data
        serialized_data = {
            "version": self.file_format_version,
            "experiments": {},
            "nodes": [],
            "edges": [],
            "node_positions": {},
            "experiment_node_map": {}
        }

        # Serialize experiments
        for exp_id, experiment in self.experiment_manager.experiments.items():
            # Create structure for storing all required experiment data
            exp_data = {
                "id": experiment.id,
                "name": experiment.name,
                "description": experiment.description,
                "task": experiment.task,
                "is_finished": experiment.is_finished,
                "train_time": experiment.train_time,
                "params": experiment._params,
                "input_data_params": vars(experiment.input_data_params),
                "parent_id": experiment.parent.id if experiment.parent else None,
                "type": "neural_network" if isinstance(experiment, GenericNeuralNetworkExperiment) else "standard"
            }

            # Add metrics data if experiment is finished
            if experiment.is_finished:
                exp_data["train_metrics"] = experiment.train_metrics
                exp_data["test_metrics"] = experiment.test_metrics

            # Save experiment data
            serialized_data["experiments"][exp_id] = exp_data

        # Serialize node data
        for node in self.node_controller.nodes:
            node_data = {
                "id": node.id,
                "name": node.get_name(),
                "position": (node.pos().x(), node.pos().y())
            }
            serialized_data["nodes"].append(node_data)

            # Save position separately for easy restoration
            serialized_data["node_positions"][node.id] = (node.pos().x(), node.pos().y())

            # Link node to experiment
            serialized_data["experiment_node_map"][node.id] = node.id  # In this case node ID = experiment ID

        # Serialize node connections
        for edge in self.node_controller.edges:
            edge_data = {
                "source_id": edge.source_node.id,
                "target_id": edge.target_node.id
            }
            serialized_data["edges"].append(edge_data)

        return serialized_data

    def _restore_from_serialization_data(self, data: Dict) -> None:
        """
        Restores workspace from serialized data.

        Args:
            data: Dictionary with serialized data
        """
        # Check file format version
        if "version" not in data or data["version"] != self.file_format_version:
            print(f"Warning: File format version ({data.get('version', 'unknown')}) "
                  f"differs from current ({self.file_format_version})")

        # Clear current workspace
        self._clear_current_workspace()

        # Restore nodes
        node_map = {}  # For storing mapping: old ID -> new node
        for node_data in data["nodes"]:
            # Create new node
            node = self.node_controller.create_node(
                x=node_data["position"][0],
                y=node_data["position"][1]
            )
            node.set_name(node_data["name"])

            # Save ID mapping
            node_map[node_data["id"]] = node

            # Explicitly set node position
            node.setPos(QPointF(node_data["position"][0], node_data["position"][1]))

        # Restore experiments
        experiment_map = {}  # old ID -> new experiment
        for exp_id, exp_data in data["experiments"].items():
            # Determine experiment type
            if exp_data["type"] == "neural_network":
                # Create neural network experiment
                experiment = GenericNeuralNetworkExperiment(
                    id=node_map[exp_id].id,  # Use new node ID
                    task=exp_data["task"],
                    model=None,  # Will be restored later
                    params=exp_data["params"]
                )
            else:
                # Create standard experiment
                experiment = Experiment(
                    id=node_map[exp_id].id,  # Use new node ID
                    task=exp_data["task"],
                    model=None,  # Will be restored later
                    params=exp_data["params"]
                )

            # Restore basic attributes
            experiment._name = exp_data["name"]
            experiment.description = exp_data["description"]
            experiment.is_finished = exp_data["is_finished"]
            experiment.train_time = exp_data["train_time"]

            # Restore input data parameters
            self._restore_input_data_params(experiment, exp_data["input_data_params"])

            # Restore metrics if experiment is finished
            if experiment.is_finished and "train_metrics" in exp_data:
                experiment.train_metrics = exp_data["train_metrics"]
                experiment.test_metrics = exp_data["test_metrics"]

            # Save experiment to manager
            self.experiment_manager.experiments[experiment.id] = experiment

            # Save ID mapping
            experiment_map[int(exp_id)] = experiment

        # Restore experiment parent relationships
        for exp_id, exp_data in data["experiments"].items():
            if exp_data["parent_id"] is not None:
                # Find corresponding parent experiment
                parent_exp = experiment_map.get(exp_data["parent_id"])
                if parent_exp:
                    # Set parent experiment
                    experiment_map[int(exp_id)].parent = parent_exp
                    # Add reference to child experiment in parent
                    parent_exp.children.append(experiment_map[int(exp_id)])

        # Restore node connections
        for edge_data in data["edges"]:
            # Find corresponding nodes
            source_node = node_map.get(edge_data["source_id"])
            target_node = node_map.get(edge_data["target_id"])

            if source_node and target_node:
                # Create connection
                self.node_controller.create_edge(source_node, target_node)

        # Update scene to display all elements correctly
        if self.node_controller.scene:
            self.node_controller.scene.update()

    def _restore_input_data_params(self, experiment, params_data):
        """
        Restores input data parameters for experiment.

        Args:
            experiment: Experiment to restore parameters for
            params_data: Dictionary with parameters
        """
        # Determine parameter type based on experiment type
        if isinstance(experiment, GenericNeuralNetworkExperiment):
            params = experiment.input_data_params
        else:
            params = experiment.input_data_params

        # Restore all attributes
        for key, value in params_data.items():
            if hasattr(params, key):
                setattr(params, key, value)

    def _clear_current_workspace(self):
        """Clears current workspace"""
        # Clear experiments
        self.experiment_manager.experiments = {}

        # Clear nodes and connections from scene
        for node in list(self.node_controller.nodes):
            self.node_controller.delete_node(node)

        # Additional cleanup if needed
        self.node_controller.nodes = []
        self.node_controller.edges = []

файл task_selector_controller.py
from typing import Dict

from PyQt5.QtCore import Qt, pyqtSignal, QObject
from PyQt5.QtGui import QFont
from PyQt5.QtWidgets import QPushButton, QLabel, QVBoxLayout, QDialog, QRadioButton, QButtonGroup, QListWidget

from project.controllers.experiment_settings_dialog.neural_network_loader_tab_controller import \
    NeuralNetworkLoaderTabController
from project.logic.modules import task_names
from project.ui.experiment_settings_dialog.neural_network_load_tab import NeuralNetworkLoaderTabWidget
from project.ui.task_selector.dynamic_button_dialog import DynamicButtonDialog
from project.ui.parameter_editor_widget import ParameterEditorWidget
from project.controllers.parameter_editor_controller import ParameterEditorController

from project.logic.evaluation.task_register import TaskType, NNModelType


class TaskSelectorController(QObject):
    request_models_dict = pyqtSignal(str)

    own_nn_selected = pyqtSignal(str)
    send_ml_model = pyqtSignal(str, object, object)
    send_nn_model = pyqtSignal(object, str, str, str)

    _instance = None  # Змінна класу для зберігання екземпляра

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, sender):
        super().__init__()
        self.sender = sender
        self.selected_task: str = ""
        self.nn_loader_dialog = None
        self.nn_loader_controller = None

    def show_approach_selection(self):
        approach_dict = {
            "Класичне МН": 1,
            "Нейронна мережа": 2
        }
        approach_dialog = DynamicButtonDialog("Вибір типу експерименту", approach_dict, self.sender)
        result = approach_dialog.exec_()

        if result == 1:  # Classical ML
            self.show_learning_type_selection()
        elif result == 2:  # Neural Networks
            self.show_neural_network_selection()

    def show_learning_type_selection(self):
        learning_type_dict = {
            "Навчання із вчителем": 1,
            "Навчання без вчителя": 2
        }
        learning_type_dialog = DynamicButtonDialog("Вибір підходу до навчання", learning_type_dict, self.sender)
        result = learning_type_dialog.exec_()

        if result == 1:  # Supervised Learning
            self.show_supervised_task_selection()
        elif result == 2:  # Unsupervised Learning
            self.show_unsupervised_task_selection()

    def show_supervised_task_selection(self):
        tasks = {
            task_names.CLASSIFICATION: 1,
            task_names.REGRESSION: 2
        }
        self.show_task_selection_dialog("Вибір задачі із вчителем", tasks)

    def show_unsupervised_task_selection(self):
        tasks = {
            task_names.CLUSTERING: 1,
            task_names.DIMENSIONALITY_REDUCTION: 2,
            task_names.ANOMALY_DETECTION: 3,
            task_names.DENSITY_ESTIMATION: 4
        }
        self.show_task_selection_dialog("Вибір задачі без вчителя", tasks)

    def show_neural_network_selection(self):
        nn_types = {
            "Scikit-learn MLP models": 1,
            "Import own": 2
        }
        self.show_task_selection_dialog("Вибір типу нейромережі", nn_types)

    def show_task_selection_dialog(self, title, tasks):
        dialog = QDialog(self.sender)
        dialog.setWindowTitle(title)
        dialog.setGeometry(200, 200, 400, 500)

        layout = QVBoxLayout(dialog)

        # Button Group for Tasks
        task_group = QButtonGroup(dialog)

        for task_text, task_value in tasks.items():
            radio_btn = QRadioButton(task_text)
            radio_btn.setMinimumHeight(100)
            radio_btn.setFont(QFont('Arial', 12))
            task_group.addButton(radio_btn, task_value)
            layout.addWidget(radio_btn)

        # Confirm Button
        confirm_btn = QPushButton("Підтвердити")
        confirm_btn.clicked.connect(lambda: self.handle_task_selection(task_group, dialog))
        layout.addWidget(confirm_btn)

        dialog.exec_()

    def handle_task_selection(self, group, dialog):
        selected_button = group.checkedButton()
        selected_task = group.checkedButton().text()
        self.selected_task = selected_task

        if selected_task == "Import own":
            dialog.accept()
            self.show_nn_loader_dialog()
            return

        if selected_button:
            placeholder_dialog = QDialog(self.sender)
            placeholder_dialog.setWindowTitle("Підтвердження")
            placeholder_dialog.setGeometry(200, 200, 400, 300)

            layout = QVBoxLayout(placeholder_dialog)

            # Title with selected option
            title_label = QLabel(f"Обрано: {selected_task}")
            title_label.setAlignment(Qt.AlignCenter)
            title_label.setFont(QFont('Arial', 14))
            layout.addWidget(title_label)
            dialog.accept()

            self.request_models_dict.emit(selected_task)

    def show_nn_loader_dialog(self):
        """Show dialog for loading neural network model"""
        loader_dialog = QDialog(self.sender)
        loader_dialog.setWindowTitle("Завантаження нейромережі")
        loader_dialog.setGeometry(200, 200, 1000, 800)

        layout = QVBoxLayout(loader_dialog)

        # Create the loader widget and controller
        nn_loader_widget = NeuralNetworkLoaderTabWidget()

        # Create a dummy experiment object
        class DummyExperiment:
            def __init__(self):
                self.task = None
                self.load_type = None
                self.model_file_path = None
                self.weights_file_path = None

        dummy_experiment = DummyExperiment()

        # Create the controller
        self.nn_loader_controller = NeuralNetworkLoaderTabController(dummy_experiment, nn_loader_widget)

        # Connect the model loaded signal
        # Add close button
        buttons_layout = QVBoxLayout()
        save_btn = QPushButton("Перейти до Налаштування")
        save_btn.clicked.connect(self.on_nn_model_loaded)
        buttons_layout.addWidget(save_btn)

        # Add widget to layout
        layout.addWidget(nn_loader_widget)
        layout.addLayout(buttons_layout)

        self.nn_loader_dialog = loader_dialog
        loader_dialog.exec_()

    def on_nn_model_loaded(self, model):
        """Handle when a neural network model is loaded"""
        if self.nn_loader_controller and self.nn_loader_dialog:
            # Get information from the controller
            task = self.nn_loader_controller.experiment.task
            model_file_path = self.nn_loader_controller.experiment.model_file_path
            weights_file_path = self.nn_loader_controller.experiment.weights_file_path
            load_type = self.nn_loader_controller.experiment.load_type
            # Send data to experiment manager
            #task_name = task.value if isinstance(task, TaskType) else str(task)
            self.send_nn_model.emit(task, model_file_path, weights_file_path, load_type)

            # Close the dialog
            self.nn_loader_dialog.accept()

    def show_model_selection_dialog(self, models_dict: Dict):
        """
            Show a dialog with all models from the dictionary and return the selected model

            Args:
                models_dict: Dictionary with model names as keys and model classes as values

            Returns:
                Instance of the selected model or None if canceled
            """

        dialog = QDialog(self.sender)
        dialog.setWindowTitle("Select Model")
        dialog.setGeometry(200, 200, 500, 600)

        # Create layout
        layout = QVBoxLayout(dialog)

        # Add title
        title = QLabel("Виберіть модель")
        title.setAlignment(Qt.AlignCenter)
        title.setFont(QFont('Arial', 14, QFont.Bold))
        layout.addWidget(title)

        # Create list widget for models
        models_list = QListWidget()
        models_list.setFont(QFont('Arial', 12))

        # Sort model names for better usability
        model_names = sorted(models_dict.keys())

        # Add models to list
        for model_name in model_names:
            models_list.addItem(model_name)

        # Set minimum height for better visibility
        models_list.setMinimumHeight(400)
        layout.addWidget(models_list)

        # Add buttons
        buttons_layout = QVBoxLayout()

        select_btn = QPushButton("Обрати")
        select_btn.setFont(QFont('Arial', 12))
        select_btn.setMinimumHeight(40)

        buttons_layout.addWidget(select_btn)
        layout.addLayout(buttons_layout)

        # Initialize result
        selected_model = None

        def on_select():
            nonlocal selected_model
            current_item = models_list.currentItem()
            if current_item:
                model_name = current_item.text()
                model_class = models_dict[model_name]
                selected_model = model_class()
                dialog.accept()

        select_btn.clicked.connect(on_select)

        # Double click to select
        models_list.itemDoubleClicked.connect(lambda item: on_select())

        # Show dialog
        result = dialog.exec_()

        # Return selected model or None if canceled
        return selected_model if result == QDialog.Accepted else None

    def handle_models_dict_response(self, models_dict):
        choosen_model = (self.show_model_selection_dialog(models_dict))
        self.send_data_to_experiment_manager(choosen_model, choosen_model.get_params())

    def send_data_to_experiment_manager(self, model, params):
        print(model, params)
        self.send_ml_model.emit(self.selected_task, model, params)

файл workspace_manager.py
from typing import Dict, List, Tuple, Any, Optional
from PyQt5.QtCore import QObject, pyqtSlot, Qt
from PyQt5.QtWidgets import QGraphicsView, QWidget, QMessageBox, QFileDialog
from project.controllers.node_controller import NodeController
from project.controllers.serializer import WorkspaceSerializer
from project.logic.experiment_manager import ExperimentManager

class WorkspaceManager(QObject):
    """
    Workspace manager responsible for saving/loading program state
    and managing project files.
    """

    def __init__(self, parent=None):
        super().__init__(parent)

        # Path to current project file
        self.current_project_path: Optional[str] = None

        # Project modification status (unsaved changes)
        self.has_unsaved_changes: bool = False

        # Serializer for save/load operations
        self.serializer = WorkspaceSerializer()

        # References to other system components
        self.experiment_manager: Optional[ExperimentManager] = None
        self.node_controller: Optional[NodeController] = None
        self.work_area: Optional[QGraphicsView] = None  # Reference to work area

        # Connect serializer signals
        self._connect_serializer_signals()

    def set_experiment_manager(self, manager: ExperimentManager):
        """Sets experiment manager for WorkspaceManager."""
        self.experiment_manager = manager
        self.serializer.set_experiment_manager(manager)

    def set_node_controller(self, controller: NodeController):
        """Sets node controller for WorkspaceManager."""
        self.node_controller = controller
        self.serializer.set_node_controller(controller)

        # Connect controller signals to track changes
        self._connect_node_controller_signals()

    def set_work_area(self, work_area: QGraphicsView):
        """Sets work area for WorkspaceManager."""
        self.work_area = work_area

    def _connect_serializer_signals(self):
        """Connects serializer signals to corresponding slots."""
        self.serializer.workspace_saved.connect(self._on_workspace_saved)
        self.serializer.workspace_loaded.connect(self._on_workspace_loaded)
        self.serializer.save_error.connect(self._on_save_error)
        self.serializer.load_error.connect(self._on_load_error)

    def _connect_node_controller_signals(self):
        """Connects node controller signals to track changes."""
        if self.node_controller:
            # Track graph changes
            self.node_controller.node_created.connect(self._on_workspace_modified)
            self.node_controller.node_deleted.connect(self._on_workspace_modified)
            self.node_controller.node_renamed.connect(self._on_workspace_modified)
            self.node_controller.experiment_inherited.connect(self._on_workspace_modified)

    @pyqtSlot()
    def _on_workspace_modified(self):
        """Handler for workspace modification events."""
        self.has_unsaved_changes = True

    @pyqtSlot(str)
    def _on_workspace_saved(self, path: str):
        """Handler for successful workspace save."""
        self.current_project_path = path
        self.has_unsaved_changes = False
        print(f"Workspace successfully saved: {path}")

    @pyqtSlot(str)
    def _on_workspace_loaded(self, path: str):
        """Handler for successful workspace load."""
        self.current_project_path = path
        self.has_unsaved_changes = False
        print(f"Workspace successfully loaded: {path}")

        # Fit view to content after loading
        self.fit_view_to_content()

    @pyqtSlot(str)
    def _on_save_error(self, error_msg: str):
        """Handler for save errors."""
        print(f"Save error: {error_msg}")
        QMessageBox.critical(None, "Помилка збереження", error_msg)

    @pyqtSlot(str)
    def _on_load_error(self, error_msg: str):
        """Handler for load errors."""
        print(f"Load error: {error_msg}")
        QMessageBox.critical(None, "Помилка завантаження", error_msg)

    def new_project(self):
        """Creates new project."""
        # Check for unsaved changes
        if self.has_unsaved_changes and not self._confirm_discard_changes():
            return False

        # Clear current data
        self._clear_workspace()

        # Reset project path
        self.current_project_path = None
        self.has_unsaved_changes = False

        return True

    def save_project(self, parent_widget: Optional[QWidget] = None) -> bool:
        """
        Saves current project.

        Args:
            parent_widget: Parent widget for dialogs

        Returns:
            bool: True on success, False on error
        """
        # Check if project path exists
        if not self.current_project_path:
            return self.save_project_as(parent_widget)

        # Save project to current path
        success = self.serializer.save_workspace(self.current_project_path)
        return success

    def save_project_as(self, parent_widget: Optional[QWidget] = None) -> bool:
        """
        Saves project to new file.

        Args:
            parent_widget: Parent widget for dialogs

        Returns:
            bool: True on success, False on error
        """
        # Open file save dialog
        options = QFileDialog.Options()
        filepath, _ = QFileDialog.getSaveFileName(
            parent_widget,
            "Зберегти проект",
            "",
            "ML Project Files (*.mlproj);;All Files (*)",
            options=options
        )

        if filepath:
            # Add extension if not specified
            if not filepath.endswith('.mlproj'):
                filepath += '.mlproj'

            # Save project to selected file
            return self.serializer.save_workspace(filepath)

        return False

    def open_project(self, parent_widget: Optional[QWidget] = None) -> bool:
        """
        Opens existing project.

        Args:
            parent_widget: Parent widget for dialogs

        Returns:
            bool: True on success, False on error
        """
        # Check for unsaved changes
        if self.has_unsaved_changes and not self._confirm_discard_changes():
            return False

        # Open file load dialog
        options = QFileDialog.Options()
        filepath, _ = QFileDialog.getOpenFileName(
            parent_widget,
            "Відкрити проект",
            "",
            "ML Project Files (*.mlproj);;All Files (*)",
            options=options
        )

        if filepath:
            # Load project from selected file
            return self.serializer.load_workspace(filepath)

        return False

    def fit_view_to_content(self):
        """Fits view to all scene items."""
        if self.work_area and self.node_controller and self.node_controller.scene:
            # Do nothing if no items exist
            if not self.node_controller.nodes:
                return

            # Calculate bounding rectangle for all nodes
            rect = None
            for node in self.node_controller.nodes:
                node_rect = node.sceneBoundingRect()
                if rect is None:
                    rect = node_rect
                else:
                    rect = rect.united(node_rect)

            # Add margin around items
            if rect:
                margin = 50  # Pixel margin
                rect = rect.adjusted(-margin, -margin, margin, margin)

                # Fit view to this rectangle
                self.work_area.fitInView(rect, Qt.KeepAspectRatio)

    def _confirm_discard_changes(self) -> bool:
        """
        Confirms discarding unsaved changes with user.

        Returns:
            bool: True if user confirmed discard, False otherwise
        """
        reply = QMessageBox.question(
            None,
            "Незбережені зміни",
            "Є незбережені зміни. Бажаєте зберегти їх перед продовженням?",
            QMessageBox.Save | QMessageBox.Discard | QMessageBox.Cancel,
            QMessageBox.Save
        )

        if reply == QMessageBox.Save:
            # Save changes
            return self.save_project()
        elif reply == QMessageBox.Discard:
            # Discard changes
            return True
        else:  # QMessageBox.Cancel
            # Cancel action
            return False

    def _clear_workspace(self):
        """Clears workspace."""
        # Clear experiments in manager
        if self.experiment_manager:
            self.experiment_manager.experiments = {}

        # Clear nodes on scene
        if self.node_controller:
            for node in list(self.node_controller.nodes):
                self.node_controller.delete_node(node)

файл __init__.py







